@misc{ainslieGQATrainingGeneralized2023,
  title = {{{GQA}}: {{Training Generalized Multi-Query Transformer Models}} from {{Multi-Head Checkpoints}}},
  shorttitle = {{{GQA}}},
  author = {Ainslie, Joshua and {Lee-Thorp}, James and {de Jong}, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-09-04},
  abstract = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5\% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.},
  howpublished = {https://arxiv.org/abs/2305.13245v3},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/AW7N2W3X/Ainslie et al. - 2023 - GQA Training Generalized Multi-Query Transformer .pdf}
}

@article{andersonVariationPhonemeInventories2023,
  title = {Variation in Phoneme Inventories: Quantifying the Problem and Improving Comparability},
  shorttitle = {Variation in Phoneme Inventories},
  author = {Anderson, Cormac and Tresoldi, Tiago and Greenhill, Simon J and Forkel, Robert and Gray, Russell and List, Johann-Mattis},
  year = {2023},
  month = jul,
  journal = {Journal of Language Evolution},
  volume = {8},
  number = {2},
  pages = {149--168},
  issn = {2058-458X},
  doi = {10.1093/jole/lzad011},
  urldate = {2024-09-01},
  abstract = {For over a century, the phoneme has played a central role in linguistic research. In recent years, collections of phoneme inventories, originally designed for cross-linguistic purposes, have increasingly been used in comparative studies involving neighbouring disciplines. Despite the extended application of this type of data, there has been no research into its comparability or tests of its reliability. In this study, we carry out a systematic comparison of nine popular phoneme inventory collections. We render them comparable by linking them to standardised formats for the handling of cross-linguistic datasets, develop new measures to test both size and similarity, and release the organised data in supplementary material. We find considerable differences in inventories supposedly representing the same language variety, both in terms of size and transcriptional choices. While some of these differences appear to be predictable, reflecting design decisions in the different collections, much of the observed variation is unsystematic. These results should sound a note of caution for comparative studies based on phoneme inventories, which we suggest need to take the question of comparability more seriously. We make a number of proposals for improving the comparability of phoneme inventories.},
  file = {/Users/ajaynarayanan/Zotero/storage/XHWUK2HH/Anderson et al. - 2023 - Variation in phoneme inventories quantifying the .pdf;/Users/ajaynarayanan/Zotero/storage/TCRPPG5R/7450385.html}
}

@misc{AnswerIPAPhonemes2012,
  title = {Answer to "{{IPA}} for Phonemes - Does This Make Sense at All?"},
  shorttitle = {Answer to "{{IPA}} for Phonemes - Does This Make Sense at All?},
  year = {2012},
  month = nov,
  journal = {Linguistics Stack Exchange},
  urldate = {2024-10-18},
  file = {/Users/ajaynarayanan/Zotero/storage/IJ9548Y6/ipa-for-phonemes-does-this-make-sense-at-all.html}
}

@inproceedings{aroraContextualEmbeddingsWhen2020,
  title = {Contextual {{Embeddings}}: {{When Are They Worth It}}?},
  shorttitle = {Contextual {{Embeddings}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Arora, Simran and May, Avner and Zhang, Jian and R{\'e}, Christopher},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  pages = {2650--2663},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.236},
  urldate = {2024-09-01},
  abstract = {We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline---random word embeddings---focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10\% accuracy (absolute) on benchmark tasks. Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.},
  file = {/Users/ajaynarayanan/Zotero/storage/RMBK4K7P/Arora et al. - 2020 - Contextual Embeddings When Are They Worth It.pdf}
}

@inproceedings{aulamo-etal-2020-opustools,
  title = {{{OpusTools}} and Parallel Corpus Diagnostics},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  author = {Aulamo, Mikko and Sulubacak, Umut and Virpioja, Sami and Tiedemann, J{\"o}rg},
  year = {2020},
  month = may,
  pages = {3782--3789},
  publisher = {European Language Resources Association},
  isbn = {979-10-95546-34-4},
  file = {/Users/ajaynarayanan/Zotero/storage/ZML97EEG/Aulamo et al. - 2020 - OpusTools and parallel corpus diagnostics.pdf}
}

@inproceedings{banerjeeMETEORAutomaticMetric2005,
  title = {{{METEOR}}: {{An Automatic Metric}} for {{MT Evaluation}} with {{Improved Correlation}} with {{Human Judgments}}},
  shorttitle = {{{METEOR}}},
  booktitle = {Proceedings of the {{ACL Workshop}} on {{Intrinsic}} and {{Extrinsic Evaluation Measures}} for {{Machine Translation}} and/or {{Summarization}}},
  author = {Banerjee, Satanjeev and Lavie, Alon},
  editor = {Goldstein, Jade and Lavie, Alon and Lin, Chin-Yew and Voss, Clare},
  year = {2005},
  month = jun,
  pages = {65--72},
  publisher = {Association for Computational Linguistics},
  address = {Ann Arbor, Michigan},
  urldate = {2025-04-02},
  file = {/Users/ajaynarayanan/Zotero/storage/PTWEZ7EK/Banerjee and Lavie - 2005 - METEOR An Automatic Metric for MT Evaluation with.pdf}
}

@misc{begusApproachingUnknownCommunication2024,
  title = {Approaching an Unknown Communication System by Latent Space Exploration and Causal Inference},
  author = {Begu{\v s}, Ga{\v s}per and Leban, Andrej and Gero, Shane},
  year = {2024},
  month = feb,
  number = {arXiv:2303.10931},
  eprint = {2303.10931},
  primaryclass = {cs, eess, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.10931},
  urldate = {2024-09-01},
  abstract = {This paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. We combine manipulation of individual latent variables to extreme values with methods inspired by causal inference into an approach we call causal disentanglement with extreme values (CDEV) and show that this method yields insights for model interpretability. With this, we can test for what properties of unknown data the model encodes as meaningful, using it to glean insight into the communication system of sperm whales (Physeter macrocephalus), one of the most intriguing and understudied animal communication systems. The network architecture used has been shown to learn meaningful representations of speech; here, it is used as a learning mechanism to decipher the properties of another vocal communication system in which case we have no ground truth. The proposed methodology suggests that sperm whales encode information using the number of clicks in a sequence, the regularity of their timing, and audio properties such as the spectral mean and the acoustic regularity of the sequences. Some of these findings are consistent with existing hypotheses, while others are proposed for the first time. We also argue that our models uncover rules that govern the structure of units in the communication system and apply them while generating innovative data not shown during training. This paper suggests that an interpretation of the outputs of deep neural networks with causal inference methodology can be a viable strategy for approaching data about which little is known and presents another case of how deep learning can limit the hypothesis space. Finally, the proposed approach can be extended to other architectures and datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/UA8A2AGF/Beguš et al. - 2024 - Approaching an unknown communication system by lat.pdf;/Users/ajaynarayanan/Zotero/storage/QXWDY6SF/2303.html}
}

@misc{begusLargeLinguisticModels2023,
  title = {Large {{Linguistic Models}}: {{Analyzing}} Theoretical Linguistic Abilities of {{LLMs}}},
  shorttitle = {Large {{Linguistic Models}}},
  author = {Begu{\v s}, Ga{\v s}per and D{\k a}bkowski, Maksymilian and Rhodes, Ryan},
  year = {2023},
  month = aug,
  number = {arXiv:2305.00948},
  eprint = {2305.00948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.00948},
  urldate = {2024-09-01},
  abstract = {The performance of large language models (LLMs) has recently improved to the point where the models can perform well on many language tasks. We show here that for the first time, the models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. In this paper, we probe into GPT-4's metalinguistic capabilities by focusing on three subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry also exemplifies behavioral interpretability of deep learning, where models' representations are accessed by explicit prompting rather than internal representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/4P6HZJ3R/Beguš et al. - 2023 - Large Linguistic Models Analyzing theoretical lin.pdf;/Users/ajaynarayanan/Zotero/storage/73FUXCM9/2305.html}
}

@misc{bengioRepresentationLearningReview2014,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2014},
  month = apr,
  number = {arXiv:1206.5538},
  eprint = {1206.5538},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1206.5538},
  urldate = {2025-04-21},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/6JQTGQ9R/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf;/Users/ajaynarayanan/Zotero/storage/3KXU7ETY/1206.html}
}

@misc{bergerVisuallyAnalyzingContextualized2020,
  title = {Visually {{Analyzing Contextualized Embeddings}}},
  author = {Berger, Matthew},
  year = {2020},
  month = sep,
  number = {arXiv:2009.02554},
  eprint = {2009.02554},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.02554},
  urldate = {2024-09-01},
  abstract = {In this paper we introduce a method for visually analyzing contextualized embeddings produced by deep neural network-based language models. Our approach is inspired by linguistic probes for natural language processing, where tasks are designed to probe language models for linguistic structure, such as parts-of-speech and named entities. These approaches are largely confirmatory, however, only enabling a user to test for information known a priori. In this work, we eschew supervised probing tasks, and advocate for unsupervised probes, coupled with visual exploration techniques, to assess what is learned by language models. Specifically, we cluster contextualized embeddings produced from a large text corpus, and introduce a visualization design based on this clustering and textual structure - cluster co-occurrences, cluster spans, and cluster-word membership - to help elicit the functionality of, and relationship between, individual clusters. User feedback highlights the benefits of our design in discovering different types of linguistic structures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/ajaynarayanan/Zotero/storage/EBLRZLKI/Berger - 2020 - Visually Analyzing Contextualized Embeddings.pdf;/Users/ajaynarayanan/Zotero/storage/WVU4VVL6/2009.html}
}

@misc{billsLanguageModelsCan,
  title = {Language Models Can Explain Neurons in Language Models},
  author = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskevar, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  urldate = {2024-08-30},
  howpublished = {https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html},
  file = {/Users/ajaynarayanan/Zotero/storage/Q9MM8JY4/index.html}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2025-04-21},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/YF3B2UGG/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/ajaynarayanan/Zotero/storage/WKWI4TMC/2005.html}
}

@inproceedings{caiIsotropyContextualEmbedding2020,
  title = {Isotropy in the {{Contextual Embedding Space}}: {{Clusters}} and {{Manifolds}}},
  shorttitle = {Isotropy in the {{Contextual Embedding Space}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Cai, Xingyu and Huang, Jiaji and Bian, Yuchen and Church, Kenneth},
  year = {2020},
  month = oct,
  urldate = {2024-08-27},
  abstract = {The geometric properties of contextual embedding spaces for deep language models such as BERT and ERNIE, have attracted considerable attention in recent years. Investigations on the contextual embeddings demonstrate a strong anisotropic space such that most of the vectors fall within a narrow cone, leading to high cosine similarities. It is surprising that these LMs are as successful as they are, given that most of their embedding vectors are as similar to one another as they are. In this paper, we argue that the isotropy indeed exists in the space, from a different but more constructive perspective. We identify isolated clusters and low dimensional manifolds in the contextual embedding space, and introduce tools to both qualitatively and quantitatively analyze them. We hope the study in this paper could provide insights towards a better understanding of the deep language models.},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/9FR8NDWJ/Cai et al. - 2020 - Isotropy in the Contextual Embedding Space Cluste.pdf}
}

@article{cammarataCurveDetectors2020,
  title = {Curve {{Detectors}}},
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
  year = {2020},
  month = jun,
  journal = {Distill},
  volume = {5},
  number = {6},
  pages = {10.23915/distill.00024.003},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.003},
  urldate = {2024-08-30}
}

@inproceedings{campelloDensityBasedClusteringBased2013,
  title = {Density-{{Based Clustering Based}} on {{Hierarchical Density Estimates}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
  editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  year = {2013},
  pages = {160--172},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-37456-2_14},
  abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a ``flat'' partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
  isbn = {978-3-642-37456-2},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/8BNMUX4Z/Campello et al. - 2013 - Density-Based Clustering Based on Hierarchical Den.pdf}
}

@book{chomskyMinimalistProgram2014,
  title = {{The minimalist program: 20th anniversary edition}},
  author = {Chomsky, Noam},
  year = {2014},
  month = jan,
  publisher = {The MIT Press},
  abstract = {In his foundational book, The Minimalist Program, published in 1995, Noam Chomsky offered a significant contribution to the generative tradition in linguistics. This twentieth-anniversary edition reissues this classic work with a new preface by the author. In four essays, Chomsky attempts to situate linguistic theory in the broader cognitive sciences, with the essays formulating and progressively developing the minimalist approach to linguistic theory. Building on the theory of principles and parameters and, in particular, on principles of economy of derivation and representation, the minimalist framework takes Universal Grammar as providing a unique computational system, with derivations driven by morphological properties, to which the syntactic variation of languages is also restricted. Within this theoretical framework, linguistic expressions are generated by optimally efficient derivations that must satisfy the conditions that hold on interface levels, the only levels of linguistic representation. The interface levels provide instructions to two types of performance systems, articulatory-perceptual and conceptual-intentional. All syntactic conditions, then, express properties of these interface levels, reflecting the interpretive requirements of language and keeping to very restricted conceptual resources. In the preface to this edition, Chomsky emphasizes that the minimalist approach developed in the book and in subsequent work --is a program, not a theory.--- With this book, Chomsky built on pursuits from the earliest days of generative grammar to formulate a new research program that had far-reaching implications for the field.},
  isbn = {978-0-262-52734-7},
  langid = {English (US)},
  file = {/Users/ajaynarayanan/Zotero/storage/NRKHS6TW/Chomsky - 2014 - The minimalist program 20th anniversary edition.pdf}
}

@inproceedings{clarkWhatDoesBERT2019,
  title = {What {{Does BERT Look}} at? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look}} At?},
  booktitle = {Proceedings of the 2019 {{ACL Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  editor = {Linzen, Tal and Chrupa{\l}a, Grzegorz and Belinkov, Yonatan and Hupkes, Dieuwke},
  year = {2019},
  month = aug,
  pages = {276--286},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-4828},
  urldate = {2024-09-04},
  abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  file = {/Users/ajaynarayanan/Zotero/storage/AY5SRHWR/Clark et al. - 2019 - What Does BERT Look at An Analysis of BERT's Atte.pdf}
}

@misc{conmyAutomatedCircuitDiscovery2023,
  title = {Towards {{Automated Circuit Discovery}} for {{Mechanistic Interpretability}}},
  author = {Conmy, Arthur and {Mavor-Parker}, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and {Garriga-Alonso}, Adri{\`a}},
  year = {2023},
  month = oct,
  number = {arXiv:2304.14997},
  eprint = {2304.14997},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.14997},
  urldate = {2024-10-18},
  abstract = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/JQSVSQ3R/Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf;/Users/ajaynarayanan/Zotero/storage/EKDFM9NX/2304.html}
}

@article{coverConvergentGamblingEstimate1978,
  title = {A Convergent Gambling Estimate of the Entropy of {{English}}},
  author = {Cover, T. and King, R.},
  year = {1978},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {24},
  number = {4},
  pages = {413--421},
  issn = {1557-9654},
  doi = {10.1109/TIT.1978.1055912},
  urldate = {2024-09-11},
  abstract = {In his original paper on the subject, Shannon found upper and lower bounds for the entropy of printed English based on the number of trials required for a subject to guess subsequent symbols in a given text. The guessing approach precludes asymptotic consistency of either the upper or lower bounds except for degenerate ergodic processes. Shannon's technique of guessing the next symbol is altered by having the subject place sequential bets on the next symbol of text. IfS\_ndenotes the subject's capital afternbets at27for1odds, and if it is assumed that the subject knows the underlying probability distribution for the processX, then the entropy estimate is{\textbackslash}hatH\_n(X)=(1-(1/n) {\l}og\_27S\_n) {\l}og\_2 27bits/symbol. If the subject does not know the true probability distribution for the stochastic process, then{\textbackslash}hatH\_n(X)is an asymptotic upper bound for the true entropy. IfXis stationary,E{\textbackslash}hatH\_n(X){\textbackslash}rightarrowH(X), H(X)being the true entropy of the process. Moreover, if X is ergodic, then by the Shannon-McMillan-Breiman theorem{\textbackslash}hatH\_n(X){\textbackslash}rightarrowH(X)with probability one. Preliminary indications are that English text has an entropy of approximately1.3bits/symbol, which agrees well with Shannon's estimate. In his original paper on the subject, Shannon found upper and lower bounds for the entropy of printed English based on the number of trials required for a subject to guess subsequent symbols in a given text. The guessing approach precludes asymptotic consistency of either the upper or lower bounds except for degenerate ergodic processes. Shannon's technique of guessing the next symbol is altered by having the subject place sequential bets on the next symbol of text. IfS\_ndenotes the subject's capital afternbets at27for1odds, and if it is assumed that the subject knows the underlying probability distribution for the processX, then the entropy estimate is{\textbackslash}hatH\_n(X)=(1-(1/n) {\l}og\_27S\_n) {\l}og\_2 27bits/symbol. If the subject does not know the true probability distribution for the stochastic process, then{\textbackslash}hatH\_n(X)is an asymptotic upper bound for the true entropy. IfXis stationary,E{\textbackslash}hatH\_n(X){\textbackslash}rightarrowH(X), H(X)being the true entropy of the process. Moreover, if X is ergodic, then by the Shannon-McMillan-Breiman theorem{\textbackslash}hatH\_n(X){\textbackslash}rightarrowH(X)with probability one. Preliminary indications are that English text has an entropy of approximately1.3bits/symbol, which agrees well with Shannon's estimate. In his original paper on the subject, Shannon found upper and lower bounds for the entropy of printed English based on the number of trials required for a subject to guess subsequent symbols in a given text. The guessing approach precludes asymptotic consistency of either the upper or lower bounds except for degenerate ergodic processes. Shannon's technique of guessing the next symbol is altered by having the subject place sequential bets on the next symbol of text. IfS\_ndenotes the subject's capital afternbets at27for1odds, and if it is assumed that the subject knows the underlying probability distribution for the processX, then the entropy estimate is{\textbackslash}hatH\_n(X)=(1-(1/n) {\l}og\_27S\_n) {\l}og\_2 27bits/symbol. If the subject does not know the true probability distribution for the stochastic process, then{\textbackslash}hatH\_n(X)is an asymptotic upper bound for the true entropy. IfXis stationary,E{\textbackslash}hatH\_n(X){\textbackslash}rightarrowH(X), H(X)being the true entropy of the process.Moreover, if X is ergodic, then by the Shannon-McMillan-Breiman theorem{\textbackslash}hatH\_n(X){\textbackslash}rightarrowH(X)with probability one. Preliminary indications are that English text has an entropy of approximately1.3bits/symbol, which agrees well with Shannon's estimate. In his original paper on the subject, Shannon found upper and lower bounds for the entropy of printed English based on the number of trials required for a subject to guess subsequent symbols in a given text. The guessing approach precludes asymptotic consistency of either the upper or lower bounds except for degenerate ergodic processes. Shannon's technique of guessing the next symbol is altered by having the subject place sequential bets on the next symbol of text. IfS\_ndenotes the subject's capital afternbets at27for1odds, and if it is assumed that the subject knows the underlying probability distribution for the processX, then the entropy estimate is{\textbackslash}hatH\_n(X)=(1-(1/n) {\l}og\_27S\_n) {\l}og\_2 27bits/symbol. If the subject does not know the true probability distribution for the stochastic process, then{\textbackslash}hatH\_n(X)is an asymptotic upper bound for the true entropy. IfXis stationary,E{\textbackslash}hatH\_n(X){\textbackslash}rightarrowH(X), H(X)being the true entropy of the process. Moreover, if X is ergodic, then by the Shannon-McMillan-Breiman theorem{\textbackslash}hatH\_n(X){\textbackslash}rightarrowH(X)with probability one. Preliminary indications are that English text has an entropy of approximately1.3bits/symbol, which agrees well with Shannon's estimate.},
  file = {/Users/ajaynarayanan/Zotero/storage/AITL6EZS/Cover and King - 1978 - A convergent gambling estimate of the entropy of E.pdf;/Users/ajaynarayanan/Zotero/storage/I2AHTP26/1055912.html}
}

@article{crockerComputationalPsycholinguistics,
  title = {Computational {{Psycholinguistics}}},
  author = {Crocker, Matthew W},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/HANCRLGU/Crocker - Computational Psycholinguistics.pdf}
}

@article{crockerComputationalPsycholinguisticsLecture,
  title = {Computational {{Psycholinguistics Lecture}} 9},
  author = {Crocker, Matthew W},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/IHTXBTDL/Crocker - Computational Psycholinguistics.pdf}
}

@article{crockerComputationalPsycholinguisticsLecturea,
  title = {Computational {{Psycholinguistics Lecture}} 10},
  author = {Crocker, Matthew W},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/7I99YQPC/Crocker - Computational Psycholinguistics.pdf}
}

@misc{cunninghamSparseAutoencodersFind2023,
  title = {Sparse {{Autoencoders Find Highly Interpretable Features}} in {{Language Models}}},
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  year = {2023},
  month = oct,
  number = {arXiv:2309.08600},
  eprint = {2309.08600},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.08600},
  urldate = {2024-08-27},
  abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/SGS8J5KX/Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf;/Users/ajaynarayanan/Zotero/storage/Z2Y4CYQ2/2309.html}
}

@misc{dainichiIPAPhonemesDoes2012,
  type = {Forum Post},
  title = {{{IPA}} for Phonemes - Does This Make Sense at All?},
  author = {{dainichi}},
  year = {2012},
  month = nov,
  journal = {Linguistics Stack Exchange},
  urldate = {2024-10-18},
  file = {/Users/ajaynarayanan/Zotero/storage/JRH93L54/ipa-for-phonemes-does-this-make-sense-at-all.html}
}

@article{debowskiInformationTheoryLanguage2020,
  title = {Information {{Theory}} and {{Language}}.},
  author = {D{\k e}bowski, {\L}ukasz and Bentz, Christian},
  year = {2020},
  month = apr,
  journal = {Entropy (Basel, Switzerland)},
  volume = {22},
  number = {4},
  address = {Switzerland},
  issn = {1099-4300},
  doi = {10.3390/e22040435},
  abstract = {Human language is a system of communication [...].},
  langid = {english},
  pmcid = {PMC7516908},
  pmid = {33286209},
  keywords = {complexity,criticality,entropy,language resources,mutual information,natural language,semantics,statistical language laws,statistical language models,syntax},
  file = {/Users/ajaynarayanan/Zotero/storage/8LQ4VJLS/Dębowski and Bentz - 2020 - Information Theory and Language..pdf}
}

@inproceedings{devardaEffectsSurprisalLanguages2022,
  title = {The {{Effects}} of {{Surprisal}} across {{Languages}}: {{Results}} from {{Native}} and {{Non-native Reading}}},
  shorttitle = {The {{Effects}} of {{Surprisal}} across {{Languages}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{AACL-IJCNLP}} 2022},
  author = {{de Varda}, Andrea and Marelli, Marco},
  editor = {He, Yulan and Ji, Heng and Li, Sujian and Liu, Yang and Chang, Chua-Hui},
  year = {2022},
  month = nov,
  pages = {138--144},
  publisher = {Association for Computational Linguistics},
  address = {Online only},
  urldate = {2024-10-08},
  abstract = {It is well known that the surprisal of an upcoming word, as estimated by language models, is a solid predictor of reading times (Smith and Levy, 2013). However, most of the studies that support this view are based on English and few other Germanic languages, leaving an open question as to the cross-lingual generalizability of such findings. Moreover, they tend to consider only the best-performing eye-tracking measure, which might conflate the effects of predictive and integrative processing. Furthermore, it is not clear whether prediction plays a role in non-native language processing in bilingual individuals (Gr{\"u}ter et al., 2014). We approach these problems at large scale, extracting surprisal estimates from mBERT, and assessing their psychometric predictive power on the MECO corpus, a cross-linguistic dataset of eye movement behavior in reading (Siegelman et al., 2022; Kuperman et al., 2020). We show that surprisal is a strong predictor of reading times across languages and fixation measurements, and that its effects in L2 are weaker with respect to L1.},
  file = {/Users/ajaynarayanan/Zotero/storage/9RMAJBEH/de Varda and Marelli - 2022 - The Effects of Surprisal across Languages Results.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  urldate = {2024-08-26},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  file = {/Users/ajaynarayanan/Zotero/storage/K46NB6NN/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@misc{dubeyLlamaHerdModels2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and {Al-Dahle}, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and {Garcia-Olano}, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and {van der Linde}, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and {El-Arini}, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and {Rantala-Yeary}, Lauren and {van der Maaten}, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and {de Oliveira}, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and {\c C}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzm{\'a}n, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, V{\'i}tor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
  year = {2024},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-08-27},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  howpublished = {https://arxiv.org/abs/2407.21783v2},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/DQMGQBGL/Dubey et al. - 2024 - The Llama 3 Herd of Models.pdf}
}

@article{elhage2021mathematical,
  title = {A Mathematical Framework for Transformer Circuits},
  author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2021},
  journal = {Transformer Circuits Thread},
  file = {/Users/ajaynarayanan/Zotero/storage/N8MKGMD8/index.html}
}

@article{elhage2022superposition,
  title = {Toy Models of Superposition},
  author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and {Hatfield-Dodds}, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
  year = {2022},
  journal = {Transformer Circuits Thread}
}

@misc{elhagePrivilegedBasesTransformer,
  title = {Privileged {{Bases}} in the {{Transformer Residual Stream}}},
  author = {Elhage, Nelson and Lasenby, Robert and Olah, Chris},
  urldate = {2024-08-30},
  howpublished = {https://transformer-circuits.pub/2023/privileged-basis/index.html},
  file = {/Users/ajaynarayanan/Zotero/storage/Z678F4TS/index.html}
}

@misc{engelsNotAllLanguage2024,
  title = {Not {{All Language Model Features Are Linear}}},
  author = {Engels, Joshua and Michaud, Eric J. and Liao, Isaac and Gurnee, Wes and Tegmark, Max},
  year = {2024},
  month = oct,
  number = {arXiv:2405.14860},
  eprint = {2405.14860},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.14860},
  urldate = {2024-10-18},
  abstract = {Recent work has proposed that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Next, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B. Finally, we find further circular representations by breaking down the hidden states for these tasks into interpretable components, and we examine the continuity of the days of the week feature in Mistral 7B.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/2LHSRCQF/Engels et al. - 2024 - Not All Language Model Features Are Linear.pdf;/Users/ajaynarayanan/Zotero/storage/U7TINSH8/2405.html}
}

@inproceedings{esterDensitybasedAlgorithmDiscovering1996,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  year = {1996},
  month = aug,
  series = {{{KDD}}'96},
  pages = {226--231},
  publisher = {AAAI Press},
  address = {Portland, Oregon},
  urldate = {2025-04-28},
  abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.}
}

@inproceedings{ethayarajhHowContextualAre2019,
  title = {How {{Contextual}} Are {{Contextualized Word Representations}}? {{Comparing}} the {{Geometry}} of {{BERT}}, {{ELMo}}, and {{GPT-2 Embeddings}}},
  shorttitle = {How {{Contextual}} Are {{Contextualized Word Representations}}?},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Ethayarajh, Kawin},
  editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  year = {2019},
  month = nov,
  pages = {55--65},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1006},
  urldate = {2024-10-16},
  abstract = {Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5\% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.},
  file = {/Users/ajaynarayanan/Zotero/storage/UA4TYVVK/Ethayarajh - 2019 - How Contextual are Contextualized Word Representat.pdf}
}

@article{everettSoundsPrehistoricSpeech,
  title = {The Sounds of Prehistoric Speech},
  author = {Everett, Caleb},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {376},
  number = {1824},
  pages = {20200195},
  issn = {0962-8436},
  doi = {10.1098/rstb.2020.0195},
  urldate = {2025-03-23},
  abstract = {Evidence is reviewed for widespread phonological and phonetic tendencies in contemporary languages. The evidence is based largely on the frequency of sound types in word lists and in phoneme inventories across the world's languages. The data reviewed point to likely tendencies in the languages of the Upper Palaeolithic. These tendencies include the reliance on specific nasal and voiceless stop consonants, the relative dispreference for posterior voiced consonants and the use of peripheral vowels. More tenuous hypotheses related to prehistoric languages are also reviewed. These include the propositions that such languages lacked labiodental consonants and relied more heavily on vowels, when contrasted to many contemporary languages. Such hypotheses suggest speech has adapted to subtle pressures that may in some cases vary across populations., This article is part of the theme issue `Reconstructing prehistoric languages'.},
  pmcid = {PMC8059574},
  pmid = {33745314},
  file = {/Users/ajaynarayanan/Zotero/storage/AJ3VRR8B/Everett - The sounds of prehistoric speech.pdf}
}

@article{ezugwuComprehensiveSurveyClustering2022,
  title = {A Comprehensive Survey of Clustering Algorithms: {{State-of-the-art}} Machine Learning Applications, Taxonomy, Challenges, and Future Research Prospects},
  shorttitle = {A Comprehensive Survey of Clustering Algorithms},
  author = {Ezugwu, Absalom E. and Ikotun, Abiodun M. and Oyelade, Olaide O. and Abualigah, Laith and Agushaka, Jeffery O. and Eke, Christopher I. and Akinyelu, Andronicus A.},
  year = {2022},
  month = apr,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {110},
  pages = {104743},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2022.104743},
  urldate = {2025-04-28},
  abstract = {Clustering is an essential tool in data mining research and applications. It is the subject of active research in many fields of study, such as computer science, data science, statistics, pattern recognition, artificial intelligence, and machine learning. Several clustering techniques have been proposed and implemented, and most of them successfully find excellent quality or optimal clustering results in the domains mentioned earlier. However, there has been a gradual shift in the choice of clustering methods among domain experts and practitioners alike, which is precipitated by the fact that most traditional clustering algorithms still depend on the number of clusters provided a priori. These conventional clustering algorithms cannot effectively handle real-world data clustering analysis problems where the number of clusters in data objects cannot be easily identified. Also, they cannot effectively manage problems where the optimal number of clusters for a high-dimensional dataset cannot be easily determined. Therefore, there is a need for improved, flexible, and efficient clustering techniques. Recently, a variety of efficient clustering algorithms have been proposed in the literature, and these algorithms produced good results when evaluated on real-world clustering problems. This study presents an up-to-date systematic and comprehensive review of traditional and state-of-the-art clustering techniques for different domains. This survey considers clustering from a more practical perspective. It shows the outstanding role of clustering in various disciplines, such as education, marketing, medicine, biology, and bioinformatics. It also discusses the application of clustering to different fields attracting intensive efforts among the scientific community, such as big data, artificial intelligence, and robotics. This survey paper will be beneficial for both practitioners and researchers. It will serve as a good reference point for researchers and practitioners to design improved and efficient state-of-the-art clustering algorithms.},
  keywords = {Automatic clustering,Clustering,Clustering algorithms partitioning,Data mining,Hierarchical clustering,K-Means,Optimization algorithms Machine learning,Supervised learning,Unsupervised learning},
  file = {/Users/ajaynarayanan/Zotero/storage/4WE6S9DY/Ezugwu et al. - 2022 - A comprehensive survey of clustering algorithms S.pdf;/Users/ajaynarayanan/Zotero/storage/V8XDDQMI/S095219762200046X.html}
}

@incollection{fedzechkinaMiniatureArtificialLanguage2016,
  title = {Miniature Artificial Language Learning as a Complement to Typological Data},
  booktitle = {The {{Usage-Based Study}} of {{Language Learning}} and {{Multilingualism}}},
  author = {Fedzechkina, Maryia and Newport, Elissa and Jaeger, T. Florian},
  year = {2016},
  month = jan,
  pages = {211--232},
  abstract = {As observed by linguist Joseph Greenberg (Greenberg 1963), languages across the world seem to share properties at all levels of linguistic organization. Some of these patterns are regularities in the crosslinguistic distribution of elements that hold across languages (non-implicational universals1). For example, sentential subjects almost always precede objects in declarative sentences (Greenberg 1963). Others, the so-called implicational universals, describe correlations between elements that vary together across languages: If a language has property A, then it most likely has property B. An example of such an implicational universal is the well-documented correlation between constituent order freedom and the presence of case-marking (Blake 2001; Sapir 1921): Languages with flexible constituent order often use morphological means, such as case, to mark grammatical function assignment (e.g., German, Japanese, and Russian), whereas languages with fixed constituent order typically lack case morphology (e.g., English and Mandarin).},
  file = {/Users/ajaynarayanan/Zotero/storage/EGYKE7PY/Fedzechkina et al. - 2016 - Miniature artificial language learning as a comple.pdf}
}

@misc{freestoneWordEmbeddingsRevisited2024,
  title = {Word {{Embeddings Revisited}}: {{Do LLMs Offer Something New}}?},
  shorttitle = {Word {{Embeddings Revisited}}},
  author = {Freestone, Matthew and Santu, Shubhra Kanti Karmaker},
  year = {2024},
  month = mar,
  number = {arXiv:2402.11094},
  eprint = {2402.11094},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.11094},
  urldate = {2025-02-07},
  abstract = {Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings similar to SBERT, a relatively lighter classical model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/FLZ3ZTQN/Freestone and Santu - 2024 - Word Embeddings Revisited Do LLMs Offer Something.pdf;/Users/ajaynarayanan/Zotero/storage/XDIGPDZC/2402.html}
}

@article{futrellInformationTheoryBridge2022,
  title = {Information Theory as a Bridge between Language Function and Language Form},
  author = {Futrell, Richard and Hahn, Michael},
  year = {2022},
  journal = {Frontiers in Communication},
  volume = {7},
  issn = {2297-900X},
  doi = {10.3389/fcomm.2022.657725},
  abstract = {{\textexclamdown}p{\textquestiondown}Formal and functional theories of language seem disparate, because formal theories answer the question of what a language is, while functional theories answer the question of what functions it serves. We argue that information theory provides a bridge between these two approaches, {\textexclamdown}italic{\textquestiondown}via{\textexclamdown}/italic{\textquestiondown} a principle of minimization of complexity under constraints. Synthesizing recent work, we show how information-theoretic characterizations of functional complexity lead directly to mathematical descriptions of the forms of possible languages, in terms of solutions to constrained optimization problems. We show how certain linguistic descriptive formalisms can be recovered as solutions to such problems. Furthermore, we argue that information theory lets us define complexity in a way which has minimal dependence on the choice of theory or descriptive formalism. We illustrate this principle using recently-obtained results on universals of word and morpheme order.{\textexclamdown}/p{\textquestiondown}},
  file = {/Users/ajaynarayanan/Zotero/storage/BHJ5GPGB/Futrell and Hahn - 2022 - Information theory as a bridge between language fu.pdf}
}

@article{gibsonHowEfficiencyShapes2019,
  title = {How {{Efficiency Shapes Human Language}}},
  author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P. and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  year = {2019},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {5},
  pages = {389--407},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.02.003},
  urldate = {2024-09-09},
  abstract = {Cognitive science applies diverse tools and perspectives to study human language. Recently, an exciting body of work has examined linguistic phenomena through the lens of efficiency in usage: what otherwise puzzling features of language find explanation in formal accounts of how language might be optimized for communication and learning? Here, we review studies that deploy formal tools from probability and information theory to understand how and why language works the way that it does, focusing on phenomena ranging from the lexicon through syntax. These studies show how a pervasive pressure for efficiency guides the forms of natural language and indicate that a rich future for language research lies in connecting linguistics to cognitive psychology and mathematical theories of communication and inference.},
  keywords = {communication,cross-linguistic universals,language complexity,language efficiency,language evolution,language learnability},
  file = {/Users/ajaynarayanan/Zotero/storage/IX55LQHV/Gibson et al. - 2019 - How Efficiency Shapes Human Language.pdf;/Users/ajaynarayanan/Zotero/storage/CREMS53R/S1364661319300580.html}
}

@article{givonIsomorphismGrammaticalCode1991,
  type = {{{https://doi.org/10.1075/sl.15.1.04giv}}},
  title = {Isomorphism in the {{Grammatical Code}}: {{Cognitive}} and {{Biological Considerations}}},
  author = {Giv{\'o}n, T.},
  year = {1991},
  journal = {Studies in Language. International Journal sponsored by the Foundation ``Foundations of Language''},
  volume = {15},
  number = {1},
  pages = {85--114},
  publisher = {John Benjamins},
  isbn = {0378-4177}
}

@misc{GrammarIthkuilLanguage,
  title = {A {{Grammar}} of the {{Ithkuil Language}} - {{Introduction}}},
  urldate = {2024-09-06},
  howpublished = {https://www.ithkuil.net/00\_intro.html},
  file = {/Users/ajaynarayanan/Zotero/storage/S98ECCJW/00_intro.html}
}

@article{gutierrez-vasquesLanguagesLookingGlass2023,
  title = {Languages {{Through}} the {{Looking Glass}} of {{BPE Compression}}},
  author = {{Gutierrez-Vasques}, Ximena and Bentz, Christian and Samard{\v z}i{\'c}, Tanja},
  year = {2023},
  month = dec,
  journal = {Computational Linguistics},
  volume = {49},
  number = {4},
  pages = {943--1001},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00489},
  urldate = {2024-12-05},
  abstract = {Byte-pair encoding (BPE) is widely used in NLP for performing subword tokenization. It uncovers redundant patterns for compressing the data, and hence alleviates the sparsity problem in downstream applications. Subwords discovered during the first merge operations tend to have the most substantial impact on the compression of texts. However, the structural underpinnings of this effect have not been analyzed cross-linguistically. We conduct in-depth analyses across 47 typologically diverse languages and three parallel corpora, and thereby show that the types of recurrent patterns that have the strongest impact on compression are an indicator of morphological typology. For languages with richer inflectional morphology there is a preference for highly productive subwords on the early merges, while for languages with less inflectional morphology, idiosyncratic subwords are more prominent. Both types of patterns contribute to efficient compression. Counter to the common perception that BPE subwords are not linguistically relevant, we find patterns across languages that resemble those described in traditional typology. We thus propose a novel way to characterize languages according to their BPE subword properties, inspired by the notion of morphological productivity in linguistics. This allows us to have language vectors that encode typological knowledge induced from raw text. Our approach is easily applicable to a wider range of languages and texts, as it does not require annotated data or any external linguistic knowledge. We discuss its potential contributions to quantitative typology and multilingual NLP.},
  file = {/Users/ajaynarayanan/Zotero/storage/MH4WSANM/Gutierrez-Vasques et al. - 2023 - Languages Through the Looking Glass of BPE Compres.pdf;/Users/ajaynarayanan/Zotero/storage/SU8YHZZ2/Languages-Through-the-Looking-Glass-of-BPE.html}
}

@incollection{haCostBenefit2010,
  title = {Cost-Benefit Analysis in Animal Behavior.},
  author = {Ha, Renee},
  year = {2010},
  month = jan,
  pages = {402--405},
  file = {/Users/ajaynarayanan/Zotero/storage/MKTZKHW2/(PDF) Cost-benefit analysis in animal behavior..pdf}
}

@article{hahnEstimatingPredictiveRate2019,
  title = {Estimating {{Predictive Rate}}--{{Distortion Curves}} via {{Neural Variational Inference}}},
  author = {Hahn, Michael and Futrell, Richard},
  year = {2019},
  month = jul,
  journal = {Entropy},
  volume = {21},
  number = {7},
  pages = {640},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e21070640},
  urldate = {2024-09-13},
  abstract = {The Predictive Rate--Distortion curve quantifies the trade-off between compressing information about the past of a stochastic process and predicting its future accurately. Existing estimation methods for this curve work by clustering finite sequences of observations or by utilizing analytically known causal states. Neither type of approach scales to processes such as natural languages, which have large alphabets and long dependencies, and where the causal states are not known analytically. We describe Neural Predictive Rate--Distortion (NPRD), an estimation method that scales to such processes, leveraging the universal approximation capabilities of neural networks. Taking only time series data as input, the method computes a variational bound on the Predictive Rate--Distortion curve. We validate the method on processes where Predictive Rate--Distortion is analytically known. As an application, we provide bounds on the Predictive Rate--Distortion of natural language, improving on bounds provided by clustering sequences. Based on the results, we argue that the Predictive Rate--Distortion curve is more useful than the usual notion of statistical complexity for characterizing highly complex processes such as natural language.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {information bottleneck,natural language,neural variational inference,Predictive Rate-Distortion},
  file = {/Users/ajaynarayanan/Zotero/storage/EHLR8T63/Hahn and Futrell - 2019 - Estimating Predictive Rate–Distortion Curves via N.pdf}
}

@article{hahnModelingWordMorpheme2021,
  title = {Modeling Word and Morpheme Order in Natural Language as an Efficient Trade-off of Memory and Surprisal},
  author = {Hahn, Michael and Degen, Judith and Futrell, Richard},
  year = {2021},
  journal = {Psychological Review},
  volume = {128},
  number = {4},
  pages = {726--756},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/rev0000269},
  abstract = {Memory limitations are known to constrain language comprehension and production, and have been argued to account for crosslinguistic word order regularities. However, a systematic assessment of the role of memory limitations in language structure has proven elusive, in part because it is hard to extract precise large-scale quantitative generalizations about language from existing mechanistic models of memory use in sentence processing. We provide an architecture-independent information-theoretic formalization of memory limitations which enables a simple calculation of the memory efficiency of languages. Our notion of memory efficiency is based on the idea of a memory--surprisal trade-off: A certain level of average surprisal per word can only be achieved at the cost of storing some amount of information about the past context. Based on this notion of memory usage, we advance the Efficient Trade-off Hypothesis: The order of elements in natural language is under pressure to enable favorable memory--surprisal trade-offs. We derive that languages enable more efficient trade-offs when they exhibit information locality: When predictive information about an element is concentrated in its recent past. We provide empirical evidence from three test domains in support of the Efficient Trade-off Hypothesis: A reanalysis of a miniature artificial language learning experiment, a large-scale study of word order in corpora of 54 languages, and an analysis of morpheme order in two agglutinative languages. These results suggest that principles of order in natural language can be explained via highly generic cognitively motivated principles and lend support to efficiency-based models of the structure of human language. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Hypothesis Testing,Information Theory,Memory,Morphemes,Natural Language,Prediction,Sentence Comprehension,Simulation},
  file = {/Users/ajaynarayanan/Zotero/storage/IHQCDXC5/Hahn et al. - 2021 - Modeling word and morpheme order in natural langua.pdf;/Users/ajaynarayanan/Zotero/storage/C77ZWGRQ/2021-31510-001.html}
}

@inproceedings{haleProbabilisticEarleyParser2001,
  title = {A {{Probabilistic Earley Parser}} as a {{Psycholinguistic Model}}},
  booktitle = {Second {{Meeting}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Hale, John},
  year = {2001},
  urldate = {2024-11-05},
  file = {/Users/ajaynarayanan/Zotero/storage/7MUEKYZL/Hale - 2001 - A Probabilistic Earley Parser as a Psycholinguisti.pdf}
}

@book{halliday2003linguistics,
  title = {On Language and Linguistics},
  author = {Halliday, M. A. K.},
  year = {2003},
  series = {Collected Works of {{M}}.{{A}}.{{K}}. Halliday},
  publisher = {Continuum},
  abstract = {The third volume in the collected works of Professor M.A.K. Halliday, On Language and Linguistics, includes eighteen papers exploring different aspects of language from a systemic functional perspective. The papers are organized into three sections: the place of linguistics as a discipline; linguistics and language; and language as social semiotic. In addition, there is a new work from Professor Halliday, entitled "The architecture of language", in which he focuses on the assumptions or working hypotheses that enabled him to explore important questions about this massive semiotic power called 'language'.},
  isbn = {978-0-8264-5869-8},
  langid = {english},
  keywords = {Language and languages,Linguistics},
  file = {/Users/ajaynarayanan/Zotero/storage/FF9PZREN/Halliday - 2003 - On language and linguistics.pdf}
}

@book{handbookIPA1999,
  title = {Handbook of the {{International Phonetic Association}} : A Guide to the Use of the International Phonetic Alphabet},
  year = {1999},
  edition = {1. publ.},
  publisher = {Cambridge Univ. Press},
  address = {Cambridge u.a.},
  file = {/Users/ajaynarayanan/Zotero/storage/EU2H8KST/1999 - Handbook of the International Phonetic Association.pdf}
}

@article{harrisDistributionalStructure1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  year = {1954},
  month = aug,
  journal = {WORD},
  publisher = {Routledge},
  issn = {0043-7956},
  urldate = {2025-04-21},
  abstract = {Published in WORD (Vol. 10, No. 2-3, 1954)},
  copyright = {{\copyright} 1954 Taylor and Francis Group, LLC},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/B5E7LMQU/00437956.1954.html}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2024-08-28},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/IK58PHKV/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/Users/ajaynarayanan/Zotero/storage/ZMYMQ7FH/2106.html}
}

@article{Ithkuil2024,
  title = {Ithkuil},
  year = {2024},
  month = aug,
  journal = {Wikipedia},
  urldate = {2024-09-06},
  abstract = {Ithkuil is an experimental constructed language created by John Quijada. It is designed to express more profound levels of human cognition briefly yet overtly and clearly, particularly about human categorization. It is a cross between an a priori philosophical and a logical language. It tries to minimize the vagueness and semantic ambiguity in natural human languages. Ithkuil is notable for its grammatical complexity and extensive phoneme inventory, the latter being simplified in an upcoming redesign. The name "Ithkuil" is an anglicized form of I{\c t}ku{\^i}l, which in the original form roughly meant "hypothetical representation of a language." Quijada states he did not create Ithkuil to be auxiliary or used in everyday conversations. Instead, he wanted the language for more elaborate and profound fields where more insightful thoughts are expected, such as philosophy, arts, science, and politics. Meaningful phrases or sentences can usually be expressed in Ithkuil with fewer linguistic units than natural languages. For example, the two-word Ithkuil sentence "Tram-m{\c l}{\"o}i hh{\^a}sma{\v r}p{\c t}ukt{\^o}x" can be translated into English as "On the contrary, I think it may turn out that this rugged mountain range trails off at some point." Quijada deems his creation as too complex to have developed naturally, seeing it as an exercise in exploring how languages could function. Nevertheless, it was featured in the Language Creation Conference's 6th Conlang Relay. Four versions of the language have been publicized: the initial version in 2004, a simplified version called Ilaksh in 2007, a third version in 2011, and the current version (as of February 2023), called New Ithkuil. In 2004---and again in 2009 with Ilaksh---Ithkuil was featured in the Russian-language popular science and IT magazine Computerra. In 2008, David J. Peterson awarded it the Smiley Award. In 2013, Bart{\l}omiej Kami{\'n}ski codified the language to parse complicated sentences quickly. Julien Tavernier and anonymous others have since followed suit. Since July 2015, Quijada has released several Ithkuil songs in a prog-rock style as part of the album Kaduat{\'a}n, which translates to "Wayfarers." Recently, online communities have developed in English, Russian, Mandarin, and Japanese.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1240012263},
  file = {/Users/ajaynarayanan/Zotero/storage/KL7ZFLX4/Ithkuil.html}
}

@misc{jabbarMorphPieceLinguisticTokenizer2024,
  title = {{{MorphPiece}} : {{A Linguistic Tokenizer}} for {{Large Language Models}}},
  shorttitle = {{{MorphPiece}}},
  author = {Jabbar, Haris},
  year = {2024},
  month = feb,
  number = {arXiv:2307.07262},
  eprint = {2307.07262},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.07262},
  urldate = {2024-09-04},
  abstract = {Tokenization is a critical part of modern NLP pipelines. However, contemporary tokenizers for Large Language Models are based on statistical analysis of text corpora, without much consideration to the linguistic features. I propose a linguistically motivated tokenization scheme, MorphPiece, which is based partly on morphological segmentation of the underlying text. A GPT-style causal language model trained on this tokenizer (called MorphGPT) shows comparable or superior performance on a variety of supervised and unsupervised NLP tasks, compared to the OpenAI GPT-2 model. Specifically I evaluated MorphGPT on language modeling tasks, zero-shot performance on GLUE Benchmark with various prompt templates, massive text embedding benchmark (MTEB) for supervised and unsupervised performance, and lastly with another morphological tokenization scheme (FLOTA, Hoffmann et al., 2022) and find that the model trained on MorphPiece outperforms GPT-2 on most evaluations, at times with considerable margin, despite being trained for about half the training iterations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/C4DBSUH9/Jabbar - 2024 - MorphPiece  A Linguistic Tokenizer for Large Lang.pdf;/Users/ajaynarayanan/Zotero/storage/YK84ZB7Y/2307.html}
}

@article{jainDataClustering502010,
  title = {Data Clustering: 50 Years beyond {{K-means}}},
  shorttitle = {Data Clustering},
  author = {Jain, Anil K.},
  year = {2010},
  month = jun,
  journal = {Pattern Recognition Letters},
  series = {Award Winning Papers from the 19th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  volume = {31},
  number = {8},
  pages = {651--666},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2009.09.011},
  urldate = {2025-04-28},
  abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.},
  keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
  file = {/Users/ajaynarayanan/Zotero/storage/BTJR64I9/Jain - 2010 - Data clustering 50 years beyond K-means.pdf;/Users/ajaynarayanan/Zotero/storage/UIACQBPI/S0167865509002323.html}
}

@book{jm3,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2025},
  edition = {3rd}
}

@article{kanwalZipfsLawAbbreviation2017,
  title = {Zipf's {{Law}} of {{Abbreviation}} and the {{Principle}} of {{Least Effort}}: {{Language}} Users Optimise a Miniature Lexicon for Efficient Communication},
  shorttitle = {Zipf's {{Law}} of {{Abbreviation}} and the {{Principle}} of {{Least Effort}}},
  author = {Kanwal, Jasmeen and Smith, Kenny and Culbertson, Jennifer and Kirby, Simon},
  year = {2017},
  month = aug,
  journal = {Cognition},
  volume = {165},
  pages = {45--52},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2017.05.001},
  abstract = {The linguist George Kingsley Zipf made a now classic observation about the relationship between a word's length and its frequency; the more frequent a word is, the shorter it tends to be. He claimed that this "Law of Abbreviation" is a universal structural property of language. The Law of Abbreviation has since been documented in a wide range of human languages, and extended to animal communication systems and even computer programming languages. Zipf hypothesised that this universal design feature arises as a result of individuals optimising form-meaning mappings under competing pressures to communicate accurately but also efficiently-his famous Principle of Least Effort. In this study, we use a miniature artificial language learning paradigm to provide direct experimental evidence for this explanatory hypothesis. We show that language users optimise form-meaning mappings only when pressures for accuracy and efficiency both operate during a communicative task, supporting Zipf's conjecture that the Principle of Least Effort can explain this universal feature of word length distributions.},
  langid = {english},
  pmid = {28494263},
  keywords = {Adolescent,Adult,Aged,Artificial language learning,Communication,Efficient communication,Female,Humans,Information theory,Language,Language universals,Learning,Male,Middle Aged,Models Psychological,Principle of Least Effort,Young Adult,Zipf's Law of Abbreviation},
  file = {/Users/ajaynarayanan/Zotero/storage/CQRSBN4A/Kanwal et al. - 2017 - Zipf's Law of Abbreviation and the Principle of Le.pdf}
}

@misc{kissaneInterpretingAttentionLayer2024,
  title = {Interpreting {{Attention Layer Outputs}} with {{Sparse Autoencoders}}},
  author = {Kissane, Connor and Krzyzanowski, Robert and Bloom, Joseph Isaac and Conmy, Arthur and Nanda, Neel},
  year = {2024},
  month = jun,
  number = {arXiv:2406.17759},
  eprint = {2406.17759},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.17759},
  urldate = {2024-10-18},
  abstract = {Decomposing model activations into interpretable components is a key open problem in mechanistic interpretability. Sparse autoencoders (SAEs) are a popular method for decomposing the internal activations of trained transformers into sparse, interpretable features, and have been applied to MLP layers and the residual stream. In this work we train SAEs on attention layer outputs and show that also here SAEs find a sparse, interpretable decomposition. We demonstrate this on transformers from several model families and up to 2B parameters. We perform a qualitative study of the features computed by attention layers, and find multiple families: long-range context, short-range context and induction features. We qualitatively study the role of every head in GPT-2 Small, and estimate that at least 90\% of the heads are polysemantic, i.e. have multiple unrelated roles. Further, we show that Sparse Autoencoders are a useful tool that enable researchers to explain model behavior in greater detail than prior work. For example, we explore the mystery of why models have so many seemingly redundant induction heads, use SAEs to motivate the hypothesis that some are long-prefix whereas others are short-prefix, and confirm this with more rigorous analysis. We use our SAEs to analyze the computation performed by the Indirect Object Identification circuit (Wang et al.), validating that the SAEs find causally meaningful intermediate variables, and deepening our understanding of the semantics of the circuit. We open-source the trained SAEs and a tool for exploring arbitrary prompts through the lens of Attention Output SAEs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/NF7TYTSC/Kissane et al. - 2024 - Interpreting Attention Layer Outputs with Sparse A.pdf;/Users/ajaynarayanan/Zotero/storage/7K3TWLBP/2406.html}
}

@article{koplenigQuantifyingEfficiencyWritten2021,
  title = {Quantifying the Efficiency of Written Language},
  author = {Koplenig, Alexander},
  year = {2021},
  month = may,
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {s3},
  publisher = {De Gruyter Mouton},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2019-0057},
  urldate = {2024-09-10},
  abstract = {Information theory can be used to assess how efficiently a message is transmitted on the basis of different symbolic systems. In this paper, I estimate the information-theoretic efficiency of written language for parallel text data in more than 1000 different languages, both on the level of characters and on the level of words as information encoding units. The main results show that (i)~the median efficiency is {$\sim$}29\% on the character level and {$\sim$}45\% on the word level, (ii) efficiency on both levels is strongly correlated with each other and (iii) efficiency tends to be higher for languages with more speakers.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {community size,efficiency,information theory,permutation testing,typology},
  file = {/Users/ajaynarayanan/Zotero/storage/KZJMP47X/Koplenig - 2021 - Quantifying the efficiency of written language.pdf}
}

@inproceedings{laiRACELargescaleReAding2017,
  title = {{{RACE}}: {{Large-scale ReAding Comprehension Dataset From Examinations}}},
  shorttitle = {{{RACE}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  editor = {Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian},
  year = {2017},
  month = sep,
  pages = {785--794},
  publisher = {Association for Computational Linguistics},
  address = {Copenhagen, Denmark},
  doi = {10.18653/v1/D17-1082},
  urldate = {2025-04-04},
  abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/ glai1/data/race/and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
  file = {/Users/ajaynarayanan/Zotero/storage/IU3CTHLW/Lai et al. - 2017 - RACE Large-scale ReAding Comprehension Dataset Fr.pdf}
}

@article{lappinAssessingStrengthsWeaknesses2024,
  title = {Assessing the {{Strengths}} and {{Weaknesses}} of {{Large Language Models}}},
  author = {Lappin, Shalom},
  year = {2024},
  month = mar,
  journal = {Journal of Logic, Language and Information},
  volume = {33},
  number = {1},
  pages = {9--20},
  issn = {1572-9583},
  doi = {10.1007/s10849-023-09409-x},
  urldate = {2024-09-01},
  abstract = {The transformers that drive chatbots and other AI systems constitute large language models (LLMs). These are currently the focus of a lively discussion in both the scientific literature and the popular media. This discussion ranges from hyperbolic claims that attribute general intelligence and sentience to LLMs, to the skeptical view that these devices are no more than ``stochastic parrots''. I present an overview of some of the weak arguments that have been presented against LLMs, and I consider several of the more compelling criticisms of these devices. The former significantly underestimate the capacity of transformers to achieve subtle inductive inferences required for high levels of performance on complex, cognitively significant tasks. In some instances, these arguments misconstrue the nature of deep learning. The latter criticisms identify significant limitations in the way in which transformers learn and represent patterns in data. They also point out important differences between the procedures through which deep neural networks and humans acquire knowledge of natural language. It is necessary to look carefully at both sets of arguments in order to achieve a balanced assessment of the potential and the limitations of LLMs.},
  langid = {english},
  keywords = {Artifical intelligence,Artificial Intelligence,Deep learning,Natural language processing,Transformers},
  file = {/Users/ajaynarayanan/Zotero/storage/9IVBGX5F/Lappin - 2024 - Assessing the Strengths and Weaknesses of Large La.pdf}
}

@book{Levshina_2022,
  title = {Communicative Efficiency: {{Language}} Structure and Use},
  author = {Levshina, Natalia},
  year = {2022},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  file = {/Users/ajaynarayanan/Zotero/storage/SZCRRRYY/Levshina - 2022 - Communicative efficiency Language structure and u.pdf}
}

@article{levshinaCommunicativeEfficiencyDifferential2021,
  title = {Communicative Efficiency and Differential Case Marking: A Reverse-Engineering Approach},
  shorttitle = {Communicative Efficiency and Differential Case Marking},
  author = {Levshina, Natalia},
  year = {2021},
  month = may,
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {s3},
  publisher = {De Gruyter Mouton},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2019-0087},
  urldate = {2024-09-10},
  abstract = {The use of differential case marking of A and P has been explained in terms of efficiency (economy) and markedness. The present study tests predictions based on these accounts, using conditional probabilities of a particular feature given the syntactic role (cue availability), and conditional probabilities of a particular syntactic role given the feature in question (cue reliability). Cue availability serves as a measure of markedness, whereas cue reliability is central for the efficiency account. Similar to reverse engineering, we determine which of the probabilistic measures could have been responsible for the recurrent cross-linguistic patterns described in the literature. The probabilities are estimated from spontaneous informal dialogues in English and Russian (Indo-European), Lao (Tai-Kadai), N{\textbar}{\textbar}ng (Tuu) and Ruuli (Bantu). The analyses, which involve a series of mixed-effects Poisson models, clearly demonstrate that cue reliability matches the observed cross-linguistic patterns better than cue availability. Thus, the results support the efficiency account of differential marking.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {conditional probability,differential argument marking,efficiency,spoken corpora},
  file = {/Users/ajaynarayanan/Zotero/storage/CWN7QZ3N/Levshina - 2021 - Communicative efficiency and differential case mar.pdf}
}

@article{levshinaEfficiencyHumanLanguages,
  title = {Efficiency in Human Languages: {{Corpus}} Evidence for Universal Principles},
  shorttitle = {Efficiency in Human Languages},
  author = {Levshina, Natalia and Moran, Steven},
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {Suppl3},
  pages = {20200081},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2020-0081},
  urldate = {2024-09-09},
  abstract = {Over the last few years, there has been a growing interest in communicative efficiency. It has been argued that language users act efficiently, saving effort for processing and articulation, and that language structure and use reflect this tendency. The emergence of new corpus data has brought to life numerous studies on efficient language use in the lexicon, in morphosyntax, and in discourse and phonology in different languages. In this introductory paper, we discuss communicative efficiency in human languages, focusing on evidence of efficient language use found in multilingual corpora. The evidence suggests that efficiency is a universal feature of human language. We provide an overview of different manifestations of efficiency on different levels of language structure, and we discuss the major questions and findings so far, some of which are addressed for the first time in the contributions in this special collection.},
  pmcid = {PMC9052279},
  pmid = {35879989},
  file = {/Users/ajaynarayanan/Zotero/storage/H2RXJHYI/Levshina and Moran - Efficiency in human languages Corpus evidence for.pdf}
}

@article{levshinaFrequencyInformativityWord2022,
  title = {Frequency, {{Informativity}} and {{Word Length}}: {{Insights}} from {{Typologically Diverse Corpora}}},
  shorttitle = {Frequency, {{Informativity}} and {{Word Length}}},
  author = {Levshina, Natalia},
  year = {2022},
  month = feb,
  journal = {Entropy},
  volume = {24},
  number = {2},
  pages = {280},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e24020280},
  urldate = {2024-10-08},
  abstract = {Zipf's law of abbreviation, which posits a negative correlation between word frequency and length, is one of the most famous and robust cross-linguistic generalizations. At the same time, it has been shown that contextual informativity (average surprisal given previous context) is more strongly correlated with word length, although this tendency is not observed consistently, depending on several methodological choices. The present study examines a more diverse sample of languages than the previous studies (Arabic, Finnish, Hungarian, Indonesian, Russian, Spanish and Turkish). I use large web-based corpora from the Leipzig Corpora Collection to estimate word lengths in UTF-8 characters and in phonemes (for some of the languages), as well as word frequency, informativity given previous word and informativity given next word, applying different methods of bigrams processing. The results show different correlations between word length and the corpus-based measure for different languages. I argue that these differences can be explained by the properties of noun phrases in a language, most importantly, by the order of heads and modifiers and their relative morphological complexity, as well as by orthographic conventions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {corpora,frequency,informativity,linguistic typology,n-grams,Zipf's law of abbreviation},
  file = {/Users/ajaynarayanan/Zotero/storage/LCPUG8JG/Levshina - 2022 - Frequency, Informativity and Word Length Insights.pdf}
}

@article{levyExpectationbasedSyntacticComprehension2008,
  title = {Expectation-Based Syntactic Comprehension},
  author = {Levy, Roger},
  year = {2008},
  month = mar,
  journal = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  urldate = {2024-11-05},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159--166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  keywords = {Frequency,Information theory,Parsing,Prediction,Sentence processing,Syntactic complexity,Syntax,Word order},
  file = {/Users/ajaynarayanan/Zotero/storage/AWR2LBYV/Levy - 2008 - Expectation-based syntactic comprehension.pdf;/Users/ajaynarayanan/Zotero/storage/UDLAPJMU/S0010027707001436.html}
}

@article{LexicalDensity2023,
  title = {Lexical Density},
  year = {2023},
  month = jul,
  journal = {Wikipedia},
  urldate = {2024-10-08},
  abstract = {Lexical density is a concept in computational linguistics that measures the structure and complexity of human communication in a language. Lexical density estimates the linguistic complexity in a written or spoken composition from the functional words (grammatical units) and content words (lexical units, lexemes). One method to calculate the lexical density is to compute the ratio of lexical items to the total number of words. Another method is to compute the ratio of lexical items to the number of higher structural items in a composition, such as the total number of clauses in the sentences. The lexical density for an individual evolves with age, education, communication style, circumstances, unusual injuries or medical condition, and his or her creativity. The inherent structure of a human language and one's first language may impact the lexical density of the individual's writing and speaking style. Further, human communication in the written form is generally more lexically dense than in the spoken form after the early childhood stage. The lexical density impacts the readability of a composition and the ease with which the listener or reader can comprehend a communication. The lexical density may also impact the memorability and retention of a sentence and the message.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1163967552},
  file = {/Users/ajaynarayanan/Zotero/storage/FTCU67ZS/Lexical_density.html}
}

@inproceedings{liangNewMultichoiceReading2019,
  title = {A {{New Multi-choice Reading Comprehension Dataset}} for {{Curriculum Learning}}},
  booktitle = {Proceedings of {{The Eleventh Asian Conference}} on {{Machine Learning}}},
  author = {Liang, Yichan and Li, Jianheng and Yin, Jian},
  year = {2019},
  month = oct,
  pages = {742--757},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-03-12},
  abstract = {The past few years have witnessed the rapid development of machine reading comprehension (MRC), especially the challenging sub-task, multiple-choice reading comprehension (MCRC). And the release of large scale datasets promotes the research in this field. Yet previous methods have already achieved high accuracy of the MCRC datasets, {\textbackslash}textit\{e.g.\} RACE. It's necessary to propose a more difficult dataset which needs more reasoning and inference for evaluating the understanding capability of new methods. To respond to such demand, we present RACE-C, a new multi-choice reading comprehension dataset collected from college English examinations in China. And further we integrate it with RACE-M and RACE-H, collected by \{\{Lai et~al.\}\} (\{2017\}) from middle and high school exams respectively, to extend RACE to be RACE++. Based on RACE++, we propose a three-stage curriculum learning framework, which is able to use the best of the characteristic that the difficulty level within these three sub-datasets is in ascending order. Statistics show the higher difficulty level of our collected dataset, RACE-C, compared to RACE's two sub-datasets, {\textbackslash}textit\{i.e.\}, RACE-M and RACE-H. And experimental results demonstrate that our proposed three-stage curriculum learning approach improves the performance of the machine reading comprehension model to an extent.},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/HJP4JCQP/Liang et al. - 2019 - A New Multi-choice Reading Comprehension Dataset f.pdf}
}

@inproceedings{liangNewMultichoiceReading2019a,
  title = {A {{New Multi-choice Reading Comprehension Dataset}} for {{Curriculum Learning}}},
  booktitle = {Proceedings of {{The Eleventh Asian Conference}} on {{Machine Learning}}},
  author = {Liang, Yichan and Li, Jianheng and Yin, Jian},
  year = {2019},
  month = oct,
  pages = {742--757},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-04-04},
  abstract = {The past few years have witnessed the rapid development of machine reading comprehension (MRC), especially the challenging sub-task, multiple-choice reading comprehension (MCRC). And the release of large scale datasets promotes the research in this field. Yet previous methods have already achieved high accuracy of the MCRC datasets, {\textbackslash}textit\{e.g.\} RACE. It's necessary to propose a more difficult dataset which needs more reasoning and inference for evaluating the understanding capability of new methods. To respond to such demand, we present RACE-C, a new multi-choice reading comprehension dataset collected from college English examinations in China. And further we integrate it with RACE-M and RACE-H, collected by \{\{Lai et~al.\}\} (\{2017\}) from middle and high school exams respectively, to extend RACE to be RACE++. Based on RACE++, we propose a three-stage curriculum learning framework, which is able to use the best of the characteristic that the difficulty level within these three sub-datasets is in ascending order. Statistics show the higher difficulty level of our collected dataset, RACE-C, compared to RACE's two sub-datasets, {\textbackslash}textit\{i.e.\}, RACE-M and RACE-H. And experimental results demonstrate that our proposed three-stage curriculum learning approach improves the performance of the machine reading comprehension model to an extent.},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/D87UAVAB/Liang et al. - 2019 - A New Multi-choice Reading Comprehension Dataset f.pdf}
}

@misc{libovickyHowLanguageNeutralMultilingual2019,
  title = {How {{Language-Neutral}} Is {{Multilingual BERT}}?},
  author = {Libovick{\'y}, Jind{\v r}ich and Rosa, Rudolf and Fraser, Alexander},
  year = {2019},
  month = nov,
  number = {arXiv:1911.03310},
  eprint = {1911.03310},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.03310},
  urldate = {2024-08-27},
  abstract = {Multilingual BERT (mBERT) provides sentence representations for 104 languages, which are useful for many multi-lingual tasks. Previous work probed the cross-linguality of mBERT using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the semantic properties of mBERT. We show that mBERT representations can be split into a language-specific component and a language-neutral component, and that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment and sentence retrieval but is not yet good enough for the more difficult task of MT quality estimation. Our work presents interesting challenges which must be solved to build better language-neutral representations, particularly for tasks requiring linguistic transfer of semantics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/CCX97X8K/Libovický et al. - 2019 - How Language-Neutral is Multilingual BERT.pdf;/Users/ajaynarayanan/Zotero/storage/R2LHWRWN/1911.html}
}

@inproceedings{liInvestigatingAspectFeatures2024,
  title = {Investigating {{Aspect Features}} in {{Contextualized Embeddings}} with {{Semantic Scales}} and {{Distributional Similarity}}},
  booktitle = {Proceedings of the 13th {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (*{{SEM}} 2024)},
  author = {Li, Yuxi and Chersoni, Emmanuele and Hsu, Yu-Yin},
  editor = {Bollegala, Danushka and Shwartz, Vered},
  year = {2024},
  month = jun,
  pages = {80--92},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.starsem-1.7},
  urldate = {2024-08-31},
  abstract = {Aspect, a linguistic category describing how actions and events unfold over time, is traditionally characterized by three semantic properties: stativity, durativity and telicity. In this study, we investigate whether and to what extent these properties are encoded in the verb token embeddings of the contextualized spaces of two English language models -- BERT and GPT-2. First, we propose an experiment using semantic projections to examine whether the values of the vector dimensions of annotated verbs for stativity, durativity and telicity reflect human linguistic distinctions. Second, we use distributional similarity to replicate the notorious Imperfective Paradox described by Dowty (1977), and assess whether the embedding models are sensitive to capture contextual nuances of the verb telicity. Our results show that both models encode the semantic distinctions for the aspect properties of stativity and telicity in most of their layers, while durativity is the most challenging feature. As for the Imperfective Paradox, only the embedding similarities computed with the vectors from the early layers of the BERT model align with the expected pattern.},
  file = {/Users/ajaynarayanan/Zotero/storage/RGBINKDI/Li et al. - 2024 - Investigating Aspect Features in Contextualized Em.pdf}
}

@inproceedings{linROUGEPackageAutomatic2004,
  title = {{{ROUGE}}: {{A Package}} for {{Automatic Evaluation}} of {{Summaries}}},
  shorttitle = {{{ROUGE}}},
  booktitle = {Text {{Summarization Branches Out}}},
  author = {Lin, Chin-Yew},
  year = {2004},
  month = jul,
  pages = {74--81},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain},
  urldate = {2025-04-02},
  file = {/Users/ajaynarayanan/Zotero/storage/43C34WJB/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summa.pdf}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  urldate = {2025-04-28},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\textasciicircum}bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  file = {/Users/ajaynarayanan/Zotero/storage/T2AQCI99/1056489.html}
}

@misc{lobinaWhatCanLarge2023,
  title = {What Can {{Large Language Models}} Offer to Linguists?},
  author = {Lobina, David J.},
  year = {2023},
  month = jun,
  journal = {OUPblog},
  urldate = {2024-09-01},
  abstract = {Does the recent, impressive performance of Large Language Models, such as OpenAI's ChatGPT, have any repercussions for the way in which linguists carry out their work? And what is a Language Model anyway?},
  howpublished = {https://blog.oup.com/2023/06/what-can-large-language-models-offer-to-linguists/},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/G7I7U36H/what-can-large-language-models-offer-to-linguists.html}
}

@misc{Lojban,
  title = {Lojban},
  urldate = {2024-08-29},
  howpublished = {https://mw.lojban.org/index.php?title=Lojban\&setlang=en-US},
  file = {/Users/ajaynarayanan/Zotero/storage/RPZCJ36E/index.html}
}

@misc{louDiscreteDiffusionModeling2024,
  title = {Discrete {{Diffusion Modeling}} by {{Estimating}} the {{Ratios}} of the {{Data Distribution}}},
  author = {Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
  year = {2024},
  month = jun,
  number = {arXiv:2310.16834},
  eprint = {2310.16834},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.16834},
  urldate = {2024-09-03},
  abstract = {Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by \$25\$-\$75\${\textbackslash}\%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around \$6\$-\$8{\textbackslash}times\$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with \$32{\textbackslash}times\$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/E6MJTTQP/Lou et al. - 2024 - Discrete Diffusion Modeling by Estimating the Rati.pdf;/Users/ajaynarayanan/Zotero/storage/3J4TLQT3/2310.html}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-09-04},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  file = {/Users/ajaynarayanan/Zotero/storage/HV3W8WKY/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf}
}

@article{matusevychTreesNeuralThose2022,
  title = {Trees Neural Those: {{RNNs}} Can Learn the Hierarchical Structure of Noun Phrases},
  shorttitle = {Trees Neural Those},
  author = {Matusevych, Yevgen and Culbertson, Jennifer},
  year = {2022},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {44},
  number = {44},
  urldate = {2024-09-01},
  abstract = {Humans use both linear and hierarchical representations in language processing, and the exact role of each has been debated. One domain where hierarchical processing is important is noun phrases. English noun phrases have a fixed order of prenominal modifiers: demonstratives - numerals - adjectives (these two green vases). However, when English speakers learn an artificial language with postnominal modifiers, instead of reproducing this linear order they preserve the distance between each modifier and the noun (vases green two these). This has been explained by a hierarchical homomorphism bias. Here, we investigate whether RNNs exhibit this bias. We pre-train one linear and two hierarchical models on English and expose them to a small artificial language. We then test them on noun phrases from a study with humans and find that only the hierarchical models can exhibit the bias, supporting the idea that homomorphic word order preferences arise from hierarchical, and not linear relations.},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/7TUQT7Q8/Matusevych and Culbertson - 2022 - Trees neural those RNNs can learn the hierarchica.pdf}
}

@misc{mengLocatingEditingFactual2023,
  title = {Locating and {{Editing Factual Associations}} in {{GPT}}},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2023},
  month = jan,
  number = {arXiv:2202.05262},
  eprint = {2202.05262},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.05262},
  urldate = {2024-11-18},
  abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/7SDD9GQM/Meng et al. - 2023 - Locating and Editing Factual Associations in GPT.pdf;/Users/ajaynarayanan/Zotero/storage/GVG5KQHI/2202.html}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-04-21},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
  file = {/Users/ajaynarayanan/Zotero/storage/U3BF4KBH/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf}
}

@inproceedings{millerWordNetLexicalDatabase1994,
  title = {{{WordNet}}: {{A Lexical Database}} for {{English}}},
  shorttitle = {{{WordNet}}},
  booktitle = {Human {{Language Technology}}: {{Proceedings}} of a {{Workshop}} Held at {{Plainsboro}}, {{New Jersey}}, {{March}} 8-11, 1994},
  author = {Miller, George A.},
  year = {1994},
  urldate = {2025-04-21},
  file = {/Users/ajaynarayanan/Zotero/storage/PWE57TT6/Miller - 1994 - WordNet A Lexical Database for English.pdf}
}

@misc{MonosemanticityDecomposingLanguage,
  title = {Towards {{Monosemanticity}}: {{Decomposing Language Models With Dictionary Learning}}},
  urldate = {2024-11-15},
  howpublished = {https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@article{moranCurrentResearchPhonological2023,
  title = {Current Research in Phonological Typology},
  author = {Moran, Steven and Easterday, Shelece and Grossman, Eitan},
  year = {2023},
  month = jul,
  journal = {Linguistic Typology},
  volume = {27},
  number = {2},
  pages = {223--243},
  publisher = {De Gruyter Mouton},
  issn = {1613-415X},
  doi = {10.1515/lingty-2022-0069},
  urldate = {2024-09-01},
  abstract = {Article Current research in phonological typology was published on July 1, 2023 in the journal Linguistic Typology  (volume 27, issue 2).},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/HDHACILL/Moran et al. - 2023 - Current research in phonological typology.pdf}
}

@misc{nainaniEvaluatingBrainInspiredModular2024,
  title = {Evaluating {{Brain-Inspired Modular Training}} in {{Automated Circuit Discovery}} for {{Mechanistic Interpretability}}},
  author = {Nainani, Jatin},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03646},
  eprint = {2401.03646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.03646},
  urldate = {2024-09-04},
  abstract = {Large Language Models (LLMs) have experienced a rapid rise in AI, changing a wide range of applications with their advanced capabilities. As these models become increasingly integral to decision-making, the need for thorough interpretability has never been more critical. Mechanistic Interpretability offers a pathway to this understanding by identifying and analyzing specific sub-networks or 'circuits' within these complex systems. A crucial aspect of this approach is Automated Circuit Discovery, which facilitates the study of large models like GPT4 or LLAMA in a feasible manner. In this context, our research evaluates a recent method, Brain-Inspired Modular Training (BIMT), designed to enhance the interpretability of neural networks. We demonstrate how BIMT significantly improves the efficiency and quality of Automated Circuit Discovery, overcoming the limitations of manual methods. Our comparative analysis further reveals that BIMT outperforms existing models in terms of circuit quality, discovery time, and sparsity. Additionally, we provide a comprehensive computational analysis of BIMT, including aspects such as training duration, memory allocation requirements, and inference speed. This study advances the larger objective of creating trustworthy and transparent AI systems in addition to demonstrating how well BIMT works to make neural networks easier to understand.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6},
  file = {/Users/ajaynarayanan/Zotero/storage/9LN2DSKL/Nainani - 2024 - Evaluating Brain-Inspired Modular Training in Auto.pdf;/Users/ajaynarayanan/Zotero/storage/PQBVDN3S/2401.html}
}

@misc{nandaMechInterp2022,
  title = {A Comprehensive Mechanistic Interpretability Explainer \& Glossary},
  author = {Nanda, Neel},
  year = {2022},
  month = dec,
  file = {/Users/ajaynarayanan/Zotero/storage/S3GQVFML/n2ZWtnoYHrU1s4vnFSAQ519J.html}
}

@article{naranjoCodingEfficiencyNominal2021,
  title = {Coding Efficiency in Nominal Inflection: Expectedness and Type Frequency Effects},
  shorttitle = {Coding Efficiency in Nominal Inflection},
  author = {Naranjo, Mat{\'i}as Guzm{\'a}n and Becker, Laura},
  year = {2021},
  month = may,
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {s3},
  publisher = {De Gruyter Mouton},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2019-0075},
  urldate = {2024-09-10},
  abstract = {Since (Zipf, George Kingsley. 1935. The psychobiology of language: An introduction to dynamic philology . Cambridge, MA: MIT Press; Zipf, George Kingsley. 1949. Human behavior and the principle of least effort. Journal of Consulting Psychology 13(3)), it has been known that more frequent lexical items tend to be shorter than less frequent ones, and this association between the length of an expression and its frequency has been applied to various grammatical patterns (syntactic, morphological, and phonological) and related to predictability or expectedness in the typological literature. However, the exact interactions of frequency and expectedness, their effect on shortening, and the mechanisms involved, are still not well understood. This paper proposes the Form-Expectedness Correspondence Hypothesis ( fech ), taking into account not only the frequency of expressions but their overall structure and distribution, and explores the fech in the domain of nominal inflection from a quantitative perspective.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {coding efficiency,entropy,form-expectedness correspondence hypothesis,inflection morphology,quantitative typology},
  file = {/Users/ajaynarayanan/Zotero/storage/BBJAXYVC/Naranjo and Becker - 2021 - Coding efficiency in nominal inflection expectedn.pdf}
}

@misc{neelakantanTextCodeEmbeddings2022,
  title = {Text and {{Code Embeddings}} by {{Contrastive Pre-Training}}},
  author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
  year = {2022},
  month = jan,
  number = {arXiv:2201.10005},
  eprint = {2201.10005},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.10005},
  urldate = {2024-08-25},
  abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/Y2DLV8KX/Neelakantan et al. - 2022 - Text and Code Embeddings by Contrastive Pre-Traini.pdf;/Users/ajaynarayanan/Zotero/storage/UCW4RCG4/2201.html}
}

@article{ohWhyDoesSurprisal2023,
  title = {Why {{Does Surprisal From Larger Transformer-Based Language Models Provide}} a {{Poorer Fit}} to {{Human Reading Times}}?},
  author = {Oh, Byung-Doh and Schuler, William},
  year = {2023},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {336--350},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00548},
  urldate = {2024-10-08},
  abstract = {This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to `memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.},
  file = {/Users/ajaynarayanan/Zotero/storage/YXXC7S25/Oh and Schuler - 2023 - Why Does Surprisal From Larger Transformer-Based L.pdf}
}

@article{olahBuildingBlocksInterpretability2018,
  title = {The {{Building Blocks}} of {{Interpretability}}},
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  year = {2018},
  month = mar,
  journal = {Distill},
  volume = {3},
  number = {3},
  pages = {10.23915/distill.00010},
  issn = {2476-0757},
  doi = {10.23915/distill.00010},
  urldate = {2024-08-30}
}

@article{olahFeatureVisualization2017,
  title = {Feature {{Visualization}}},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  year = {2017},
  month = nov,
  journal = {Distill},
  volume = {2},
  number = {11},
  pages = {10.23915/distill.00007},
  issn = {2476-0757},
  doi = {10.23915/distill.00007},
  urldate = {2024-08-30}
}

@article{olahZoomIntroductionCircuits2020,
  title = {Zoom {{In}}: {{An Introduction}} to {{Circuits}}},
  shorttitle = {Zoom {{In}}},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  year = {2020},
  month = mar,
  journal = {Distill},
  volume = {5},
  number = {3},
  pages = {10.23915/distill.00024.001},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.001},
  urldate = {2024-08-30}
}

@article{olsson2022context,
  title = {In-Context Learning and Induction Heads},
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2022},
  journal = {Transformer Circuits Thread},
  file = {/Users/ajaynarayanan/Zotero/storage/38QHZ7UB/index.html}
}

@misc{oneillDisentanglingDenseEmbeddings2024,
  title = {Disentangling {{Dense Embeddings}} with {{Sparse Autoencoders}}},
  author = {O'Neill, Charles and Ye, Christine and Iyer, Kartheik and Wu, John F.},
  year = {2024},
  month = aug,
  number = {arXiv:2408.00657},
  eprint = {2408.00657},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.00657},
  urldate = {2024-10-18},
  abstract = {Sparse autoencoders (SAEs) have shown promise in extracting interpretable features from complex neural networks. We present one of the first applications of SAEs to dense text embeddings from large language models, demonstrating their effectiveness in disentangling semantic concepts. By training SAEs on embeddings of over 420,000 scientific paper abstracts from computer science and astronomy, we show that the resulting sparse representations maintain semantic fidelity while offering interpretability. We analyse these learned features, exploring their behaviour across different model capacities and introducing a novel method for identifying ``feature families'' that represent related concepts at varying levels of abstraction. To demonstrate the practical utility of our approach, we show how these interpretable features can be used to precisely steer semantic search, allowing for fine-grained control over query semantics. This work bridges the gap between the semantic richness of dense embeddings and the interpretability of sparse representations. We open source our embeddings, trained sparse autoencoders, and interpreted features, as well as a web app for exploring them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/TXLRLZ4A/O'Neill et al. - 2024 - Disentangling Dense Embeddings with Sparse Autoenc.pdf;/Users/ajaynarayanan/Zotero/storage/ZHWVPMSE/2408.html}
}

@inproceedings{papineniBLEUMethodAutomatic2002,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  shorttitle = {{{BLEU}}},
  booktitle = {Proceedings of the 40th {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2002},
  month = jul,
  series = {{{ACL}} '02},
  pages = {311--318},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10.3115/1073083.1073135},
  urldate = {2025-04-02},
  abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  file = {/Users/ajaynarayanan/Zotero/storage/62ES2EVM/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine.pdf}
}

@inproceedings{penningtonGloVeGlobalVectors2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2025-04-21},
  file = {/Users/ajaynarayanan/Zotero/storage/T2PA6SGM/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf}
}

@inproceedings{petersDeepContextualizedWord2018,
  title = {Deep {{Contextualized Word Representations}}},
  shorttitle = {{{ELMo}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  year = {2018},
  month = jun,
  pages = {2227--2237},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1202},
  urldate = {2024-08-26},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  file = {/Users/ajaynarayanan/Zotero/storage/4XKS8APL/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf}
}

@misc{petersDissectingContextualWord2018,
  title = {Dissecting {{Contextual Word Embeddings}}: {{Architecture}} and {{Representation}}},
  shorttitle = {Dissecting {{Contextual Word Embeddings}}},
  author = {Peters, Matthew E. and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  year = {2018},
  month = sep,
  number = {arXiv:1808.08949},
  eprint = {1808.08949},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1808.08949},
  urldate = {2024-08-27},
  abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/TL2G4WLY/Peters et al. - 2018 - Dissecting Contextual Word Embeddings Architectur.pdf;/Users/ajaynarayanan/Zotero/storage/SI9PPKAH/1808.html}
}

@book{petersonArtLanguageInvention2015,
  title = {The Art of Language Invention : From {{Horse-Lords}} to {{Dark Elves}}, the Words behind World-Building},
  author = {Peterson, David J.},
  year = {2015},
  publisher = {Penguin Books},
  address = {New York, New York},
  chapter = {ix, 292 pages ; 22 cm},
  isbn = {978-0-14-312646-1 0-14-312646-6},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/EGVWWAA6/Peterson, David J - The Art of Language Invention_ From Horse-Lords to Dark Elves, the Words Behind World-Building-Penguin Audio_Penguin Publishing Group (2015).epub}
}

@book{phoible,
  title = {Phoible 2.0},
  editor = {Moran, Steven and McCloy, Daniel},
  year = {2019},
  publisher = {Max Planck Institute for the Science of Human History},
  address = {Jena}
}

@article{piantadosiZipfsWordFrequency2014,
  title = {Zipf's Word Frequency Law in Natural Language: {{A}} Critical Review and Future Directions},
  shorttitle = {Zipf's Word Frequency Law in Natural Language},
  author = {Piantadosi, Steven T.},
  year = {2014},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {21},
  number = {5},
  pages = {1112--1130},
  issn = {1531-5320},
  doi = {10.3758/s13423-014-0585-6},
  urldate = {2024-10-16},
  abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf's law. This article first shows that human language has a highly complex, reliable structure in the frequency distribution over and above this classic law, although prior data visualization methods have obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts or is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
  langid = {english},
  keywords = {Language,Statistics,Zipf's law},
  file = {/Users/ajaynarayanan/Zotero/storage/L3MWEA9G/Piantadosi - 2014 - Zipf’s word frequency law in natural language A c.pdf}
}

@inproceedings{pimentelSurprisalDurationTradeworlds2021,
  title = {A Surprisal--Duration Trade-off across and within the World's Languages},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Pimentel, Tiago and Meister, Clara and Salesky, Elizabeth and Teufel, Simone and Blasi, Dami{\'a}n and Cotterell, Ryan},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  year = {2021},
  month = nov,
  pages = {949--962},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.73},
  urldate = {2024-11-06},
  abstract = {While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal--duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal--duration trade-off in operation, both across and within the world's languages.},
  file = {/Users/ajaynarayanan/Zotero/storage/AL4R3WW3/Pimentel et al. - 2021 - A surprisal–duration trade-off across and within t.pdf}
}

@inproceedings{prokopidisParallelGlobalVoices2016,
  title = {Parallel {{Global Voices}}: A {{Collection}} of {{Multilingual Corpora}} with {{Citizen Media Stories}}},
  shorttitle = {Parallel {{Global Voices}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {Prokopidis, Prokopis and Papavassiliou, Vassilis and Piperidis, Stelios},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi, Sara and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  year = {2016},
  month = may,
  pages = {900--905},
  publisher = {European Language Resources Association (ELRA)},
  address = {Portoro{\v z}, Slovenia},
  urldate = {2024-09-04},
  abstract = {We present a new collection of multilingual corpora automatically created from the content available in the Global Voices websites, where volunteers have been posting and translating citizen media stories since 2004. We describe how we crawled and processed this content to generate parallel resources comprising 302.6K document pairs and 8.36M segment alignments in 756 language pairs. For some language pairs, the segment alignments in this resource are the first open examples of their kind. In an initial use of this resource, we discuss how a set of document pair detection algorithms performs on the Greek-English corpus.},
  file = {/Users/ajaynarayanan/Zotero/storage/HXUAJ4SR/Prokopidis et al. - 2016 - Parallel Global Voices a Collection of Multilingu.pdf}
}

@inproceedings{radfordImprovingLanguageUnderstanding2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  shorttitle = {{{GPT}}},
  author = {Radford, Alec and Narasimhan, Karthik},
  year = {2018},
  urldate = {2024-08-26},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  file = {/Users/ajaynarayanan/Zotero/storage/B5NEEDU5/Radford and Narasimhan - 2018 - Improving Language Understanding by Generative Pre.pdf}
}

@article{rogersGettingCloserAI2020,
  title = {Getting {{Closer}} to {{AI Complete Question Answering}}: {{A Set}} of {{Prerequisite Real Tasks}}},
  shorttitle = {Getting {{Closer}} to {{AI Complete Question Answering}}},
  author = {Rogers, Anna and Kovaleva, Olga and Downey, Matthew and Rumshisky, Anna},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8722--8731},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6398},
  urldate = {2025-03-12},
  abstract = {The recent explosion in question answering research produced a wealth of both factoid reading comprehension (RC) and commonsense reasoning datasets. Combining them presents a different kind of task: deciding not simply whether information is present in the text, but also whether a confident guess could be made for the missing information. We present QuAIL, the first RC dataset to combine text-based, world knowledge and unanswerable questions, and to provide question type annotation that would enable diagnostics of the reasoning strategies by a given QA system. QuAIL contains 15K multi-choice questions for 800 texts in 4 domains. Crucially, it offers both general and text-specific questions, unlikely to be found in pretraining data. We show that QuAIL poses substantial challenges to the current state-of-the-art systems, with a 30\% drop in accuracy compared to the most similar existing dataset.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/XAV3ZYRM/Rogers et al. - 2020 - Getting Closer to AI Complete Question Answering .pdf}
}

@misc{rogersPrimerBERTologyWhat2020a,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = {2020},
  month = nov,
  number = {arXiv:2002.12327},
  eprint = {2002.12327},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.12327},
  urldate = {2024-09-02},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/2734LVYQ/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf;/Users/ajaynarayanan/Zotero/storage/GM8E8RAF/2002.html}
}

@book{rosenfelder2010language,
  title = {The Language Construction Kit},
  author = {Rosenfelder, M.},
  year = {2010},
  publisher = {Yonagu Books},
  isbn = {978-0-9844700-0-6},
  lccn = {2010280759},
  file = {/Users/ajaynarayanan/Zotero/storage/4UDB553S/Rosenfelder - 2010 - The language construction kit.pdf}
}

@article{schnellEfficiencyDiscourseProcessing2021,
  title = {Efficiency in Discourse Processing: {{Does}} Morphosyntax Adapt to Accommodate New Referents?},
  shorttitle = {Efficiency in Discourse Processing},
  author = {Schnell, Stefan and Schiborr, Nils Norman and Haig, Geoffrey},
  year = {2021},
  month = may,
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {s3},
  publisher = {De Gruyter Mouton},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2019-0064},
  urldate = {2024-09-10},
  abstract = {The introduction of new referents into discourse has traditionally been regarded as a major challenge to language processing, for which speakers deploy specific syntactic configurations, guided by the speaker's assessment of the recipient's state of mind (`recipient design'). In this paper we probe these assumptions against discourse data from nine languages. We find little evidence for specialized syntactic configurations accommodating new referents; the only notable exception is the association of new reference with direct objects, suggests that linking new referents to already established discourse frames through a transitive construction is preferable to isolating them in an intransitive one. Where specific intransitive predicates are indeed found to host new referents, we find this to be motivated primarily by semantic considerations. Contrary to long-held assumptions, we conclude that the cognitive challenge of referent introduction is only weakly reflected in morphosyntax; instead, discourse production is most efficient when new referents are integrated seamlessly with content-driven demands of the narration.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {corpus-based typology,discourse processing,efficiency,information pressure,recipient design},
  file = {/Users/ajaynarayanan/Zotero/storage/A528IRTF/Schnell et al. - 2021 - Efficiency in discourse processing Does morphosyn.pdf}
}

@article{schreyerConstructedLanguages2021,
  type = {Journal {{Article}}},
  title = {Constructed Languages},
  author = {Schreyer, Christine},
  year = {2021},
  journal = {Annual Review of Anthropology},
  volume = {50},
  number = {Volume 50, 2021},
  pages = {327--344},
  publisher = {Annual Reviews},
  issn = {1545-4290},
  doi = {10.1146/annurev-anthro-101819-110152},
  abstract = {Constructed languages, also known as conlangs, are languages that have been purposefully created for either real-world or fictional speakers. Within this article, I provide a summary of the language creation process and how the community of conlangers, people who make languages, come to know each other\&apos;s work, as well as how language creation assignments are being adopted within university classrooms. I also explore the role of the language creator in bringing a community of speakers into existence through the invention of a language. I discuss whether speakers of a constructed language are part of a community of practice or a speech community and the implications for this distinction within anthropology. I also describe conscripts, or constructed orthographies, as well as the relationship between endangered languages and constructed languages, how invented worlds can create real-world shifts in worldview, and suggestions for new directions in research linking anthropology and constructed languages.},
  keywords = {community of practice,constructed languages,endangered languages,Esperanto,Klingon,planned languages,speech community},
  file = {/Users/ajaynarayanan/Zotero/storage/EC9L2D4D/Schreyer - 2021 - Constructed languages.pdf}
}

@inproceedings{sennrichNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  editor = {Erk, Katrin and Smith, Noah A.},
  year = {2016},
  month = aug,
  pages = {1715--1725},
  publisher = {Association for Computational Linguistics},
  address = {Berlin, Germany},
  doi = {10.18653/v1/P16-1162},
  urldate = {2024-08-27},
  file = {/Users/ajaynarayanan/Zotero/storage/XD57IKVB/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf}
}

@article{shannonPredictionEntropyPrinted1951,
  title = {Prediction and Entropy of Printed {{English}}},
  author = {Shannon, C. E.},
  year = {1951},
  month = jan,
  journal = {The Bell System Technical Journal},
  volume = {30},
  number = {1},
  pages = {50--64},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1951.tb01366.x},
  urldate = {2024-09-11},
  abstract = {A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed.},
  file = {/Users/ajaynarayanan/Zotero/storage/I8CS687A/Shannon - 1951 - Prediction and entropy of printed English.pdf;/Users/ajaynarayanan/Zotero/storage/63AXRF3Z/6773263.html}
}

@article{sharkeyInterimResearchReport2022,
  title = {[{{Interim}} Research Report] {{Taking}} Features out of Superposition with Sparse Autoencoders},
  author = {Sharkey, Lee and Braun, Dan and {beren}},
  year = {2022},
  month = dec,
  urldate = {2024-08-30},
  abstract = {We're thankful for helpful comments from Trenton Bricken, Eric Winsor, Noa Nabeshima, and Sid Black.~ {\dots}},
  langid = {english},
  file = {/Users/ajaynarayanan/Zotero/storage/FMJRA4VJ/interim-research-report-taking-features-out-of-superposition.html}
}

@misc{shazeerOutrageouslyLargeNeural2017,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = {2017},
  month = jan,
  number = {arXiv:1701.06538},
  eprint = {1701.06538},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.06538},
  urldate = {2024-09-04},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/H2SNMWWZ/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-G.pdf;/Users/ajaynarayanan/Zotero/storage/MVLX5KN8/1701.html}
}

@misc{simhiInterpretingEmbeddingSpaces2023,
  title = {Interpreting {{Embedding Spaces}} by {{Conceptualization}}},
  author = {Simhi, Adi and Markovitch, Shaul},
  year = {2023},
  month = nov,
  number = {arXiv:2209.00445},
  eprint = {2209.00445},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.00445},
  urldate = {2024-08-27},
  abstract = {One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback of this type of representation is their incomprehensibility to humans. Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model. In this paper, we present a novel method of understanding embeddings by transforming a latent embedding space into a comprehensible conceptual space. We present an algorithm for deriving a conceptual space with dynamic on-demand granularity. We devise a new evaluation method, using either human rater or LLM-based raters, to show that the conceptualized vectors indeed represent the semantics of the original latent ones. We show the use of our method for various tasks, including comparing the semantics of alternative models and tracing the layers of the LLM. The code is available online https://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/YRVI8GW2/Simhi and Markovitch - 2023 - Interpreting Embedding Spaces by Conceptualization.pdf;/Users/ajaynarayanan/Zotero/storage/ZT4H7XLA/2209.html}
}

@article{staveOptimizationMorphemeLength2021,
  title = {Optimization of Morpheme Length: A Cross-Linguistic Assessment of {{Zipf}}'s and {{Menzerath}}'s Laws},
  shorttitle = {Optimization of Morpheme Length},
  author = {Stave, Matthew and Paschen, Ludger and Pellegrino, Fran{\c c}ois and Seifart, Frank},
  year = {2021},
  month = may,
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {s3},
  publisher = {De Gruyter Mouton},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2019-0076},
  urldate = {2024-09-10},
  abstract = {Zipf's Law of Abbreviation and Menzerath's Law both make predictions about the length of linguistic units, based on corpus frequency and the length of the carrier unit. Each contributes to the efficiency of languages: for Zipf, units are more likely to be reduced when they are highly predictable, due to their frequency; for Menzerath, units are more likely to be reduced when there are more sub-units to contribute to the structural information of the carrier unit. However, it remains unclear how the two laws work together in determining unit length at a given level of linguistic structure. We examine this question regarding the length of morphemes in spoken corpora of nine typologically diverse languages drawn from the DoReCo corpus, showing that Zipf's Law is a stronger predictor, but that the two laws interact with one another. We also explore how this is affected by specific typological characteristics, such as morphological complexity.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {corpus linguistics,cross-linguistic,efficiency,language universals,Menzerath,typology,Zipf},
  file = {/Users/ajaynarayanan/Zotero/storage/FP58WN4A/Stave et al. - 2021 - Optimization of morpheme length a cross-linguisti.pdf}
}

@misc{tenneyWhatYouLearn2019,
  title = {What Do You Learn from Context? {{Probing}} for Sentence Structure in Contextualized Word Representations},
  shorttitle = {What Do You Learn from Context?},
  author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  month = may,
  number = {arXiv:1905.06316},
  eprint = {1905.06316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.06316},
  urldate = {2024-08-27},
  abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/PX859CMT/Tenney et al. - 2019 - What do you learn from context Probing for senten.pdf;/Users/ajaynarayanan/Zotero/storage/RQ6MWBZI/1905.html}
}

@misc{ToolModuleChomskys,
  title = {Tool {{Module}}: {{Chomsky}}'s {{Universal Grammar}}},
  urldate = {2024-09-01},
  howpublished = {https://thebrain.mcgill.ca/flash/capsules/outil\_rouge06.html},
  file = {/Users/ajaynarayanan/Zotero/storage/IIY4M5K3/outil_rouge06.html}
}

@book{trask2007language,
  title = {Language and Linguistics: {{The}} Key Concepts},
  author = {Trask, R.L. and Stockwell, P.},
  year = {2007},
  series = {Key Concepts Series},
  publisher = {Routledge},
  isbn = {978-0-415-41359-6},
  lccn = {2006036672},
  file = {/Users/ajaynarayanan/Zotero/storage/RVXTWEM8/Trask and Stockwell - 2007 - Language and linguistics The key concepts.pdf}
}

@book{wals,
  type = {Data Set},
  title = {{{WALS}} Online (V2020.3)},
  editor = {Dryer, Matthew S. and Haspelmath, Martin},
  year = {2013},
  publisher = {Zenodo},
  doi = {10.5281/zenodo.7385533}
}

@misc{warstadtBLiMPBenchmarkLinguistic2023,
  title = {{{BLiMP}}: {{The Benchmark}} of {{Linguistic Minimal Pairs}} for {{English}}},
  shorttitle = {{{BLiMP}}},
  author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
  year = {2023},
  month = feb,
  number = {arXiv:1912.00582},
  eprint = {1912.00582},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.00582},
  urldate = {2024-09-01},
  abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP), a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars, and aggregate human agreement with the labels is 96.4\%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs. We find that state-of-the-art models identify morphological contrasts reliably, but they struggle with semantic restrictions on the distribution of quantifiers and negative polarity items and subtle syntactic phenomena such as extraction islands.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/7SLFI5JF/Warstadt et al. - 2023 - BLiMP The Benchmark of Linguistic Minimal Pairs f.pdf;/Users/ajaynarayanan/Zotero/storage/EXNRCZRL/1912.html}
}

@misc{wuGooglesNeuralMachine2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  shorttitle = {Google's {{Neural Machine Translation System}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  month = oct,
  number = {arXiv:1609.08144},
  eprint = {1609.08144},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.08144},
  urldate = {2024-08-27},
  abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/WE7ASMTG/Wu et al. - 2016 - Google's Neural Machine Translation System Bridgi.pdf;/Users/ajaynarayanan/Zotero/storage/FVGGZ7JU/1609.html}
}

@inproceedings{xuLinearityEffectSurprisal2023,
  title = {The {{Linearity}} of the {{Effect}} of {{Surprisal}} on {{Reading Times}} across {{Languages}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Xu, Weijie and Chon, Jason and Liu, Tianran and Futrell, Richard},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {15711--15721},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.1052},
  urldate = {2024-10-08},
  abstract = {In psycholinguistics, surprisal theory posits that the amount of online processing effort expended by a human comprehender per word positively correlates with the surprisal of that word given its preceding context. In addition to this overall correlation, more importantly, the specific quantitative form taken by the processing effort as a function of surprisal offers insights into the underlying cognitive mechanisms of language processing. Focusing on English, previous studies have looked into the linearity of surprisal on reading times. Here, we extend the investigation by examining eyetracking corpora of seven languages: Danish, Dutch, English, German, Japanese, Mandarin, and Russian. We find evidence for superlinearity in some languages, but the results are highly sensitive to which language model is used to estimate surprisal.},
  file = {/Users/ajaynarayanan/Zotero/storage/RCK8AITV/Xu et al. - 2023 - The Linearity of the Effect of Surprisal on Readin.pdf}
}

@article{yadavDependencyLengthsExplain2021,
  title = {Do Dependency Lengths Explain Constraints on Crossing Dependencies?},
  author = {Yadav, Himanshu and Husain, Samar and Futrell, Richard},
  year = {2021},
  month = may,
  journal = {Linguistics Vanguard},
  volume = {7},
  number = {s3},
  publisher = {De Gruyter Mouton},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2019-0070},
  urldate = {2024-09-10},
  abstract = {In syntactic dependency trees, when arcs are drawn from syntactic heads to dependents, they rarely cross. Constraints on these crossing dependencies are critical for determining the syntactic properties of human language, because they define the position of natural language in formal language hierarchies. We study whether the apparent constraints on crossing syntactic dependencies in natural language might be explained by constraints on dependency lengths (the linear distance between heads and dependents). We compare real dependency trees from treebanks of 52 languages against baselines of random trees which are matched with the real trees in terms of their dependency lengths. We find that these baseline trees have many more crossing dependencies than real trees, indicating that a constraint on dependency lengths alone cannot explain the empirical rarity of crossing dependencies. However, we find evidence that a combined constraint on dependency length and the rate of crossing dependencies might be able to explain two of the most-studied formal restrictions on dependency trees: gap degree and well-nestedness.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {crossing dependencies,dependency length,dependency treebanks,efficiency,language processing,syntax},
  file = {/Users/ajaynarayanan/Zotero/storage/EGNVNYDY/Yadav et al. - 2021 - Do dependency lengths explain constraints on cross.pdf}
}

@inproceedings{yedetoreHowPoorStimulus2023,
  title = {How Poor Is the Stimulus? {{Evaluating}} Hierarchical Generalization in Neural Networks Trained on Child-Directed Speech},
  shorttitle = {How Poor Is the Stimulus?},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yedetore, Aditya and Linzen, Tal and Frank, Robert and McCoy, R. Thomas},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {9370--9393},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.521},
  urldate = {2024-09-01},
  abstract = {When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children's linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children's linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.},
  file = {/Users/ajaynarayanan/Zotero/storage/EUUNQ4DN/Yedetore et al. - 2023 - How poor is the stimulus Evaluating hierarchical .pdf}
}

@book{yule2020StudyLanguage,
  title = {The Study of Language},
  author = {Yule, George},
  year = {2020},
  edition = {7},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  file = {/Users/ajaynarayanan/Zotero/storage/KYWQDMF3/Yule - 2020 - The study of language.pdf}
}

@misc{yuNeuronLevelKnowledgeAttribution2024,
  title = {Neuron-{{Level Knowledge Attribution}} in {{Large Language Models}}},
  author = {Yu, Zeping and Ananiadou, Sophia},
  year = {2024},
  month = jun,
  number = {arXiv:2312.12141},
  eprint = {2312.12141},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.12141},
  urldate = {2024-08-30},
  abstract = {Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons for different outputs. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify "value neurons" directly contributing to the final prediction, we introduce a static method for identifying "query neurons" which activate these "value neurons". Finally, we apply our methods to analyze the localization of six distinct types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. We will release our data and code on github.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/DLYX8WPR/Yu and Ananiadou - 2024 - Neuron-Level Knowledge Attribution in Large Langua.pdf;/Users/ajaynarayanan/Zotero/storage/L7XB5MQV/2312.html}
}

@inproceedings{yunTransformerVisualizationDictionary2021,
  title = {Transformer Visualization via Dictionary Learning: Contextualized Embedding as a Linear Superposition of Transformer Factors},
  shorttitle = {Transformer Visualization via Dictionary Learning},
  booktitle = {Proceedings of {{Deep Learning Inside Out}} ({{DeeLIO}}): {{The}} 2nd {{Workshop}} on {{Knowledge Extraction}} and {{Integration}} for {{Deep Learning Architectures}}},
  author = {Yun, Zeyu and Chen, Yubei and Olshausen, Bruno and LeCun, Yann},
  editor = {Agirre, Eneko and Apidianaki, Marianna and Vuli{\'c}, Ivan},
  year = {2021},
  month = jun,
  pages = {1--10},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.deelio-1.1},
  urldate = {2024-08-30},
  abstract = {Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these `black boxes' as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis.},
  file = {/Users/ajaynarayanan/Zotero/storage/H5SK89LV/Yun et al. - 2021 - Transformer visualization via dictionary learning.pdf}
}

@misc{zengSurveyMachineReading2020,
  title = {A {{Survey}} on {{Machine Reading Comprehension}}: {{Tasks}}, {{Evaluation Metrics}} and {{Benchmark Datasets}}},
  shorttitle = {A {{Survey}} on {{Machine Reading Comprehension}}},
  author = {Zeng, Changchang and Li, Shaobo and Li, Qin and Hu, Jie and Hu, Jianjun},
  year = {2020},
  month = oct,
  number = {arXiv:2006.11880},
  eprint = {2006.11880},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11880},
  urldate = {2025-04-03},
  abstract = {Machine Reading Comprehension (MRC) is a challenging Natural Language Processing(NLP) research field with wide real-world applications. The great progress of this field in recent years is mainly due to the emergence of large-scale datasets and deep learning. At present, a lot of MRC models have already surpassed human performance on various benchmark datasets despite the obvious giant gap between existing MRC models and genuine human-level reading comprehension. This shows the need for improving existing datasets, evaluation metrics, and models to move current MRC models toward "real" understanding. To address the current lack of comprehensive survey of existing MRC tasks, evaluation metrics, and datasets, herein, (1) we analyze 57 MRC tasks and datasets and propose a more precise classification method of MRC tasks with 4 different attributes; (2) we summarized 9 evaluation metrics of MRC tasks, 7 attributes and 10 characteristics of MRC datasets; (3) We also discuss key open issues in MRC research and highlighted future research directions. In addition, we have collected, organized, and published our data on the companion website(https://mrc-datasets.github.io/) where MRC researchers could directly access each MRC dataset, papers, baseline projects, and the leaderboard.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/FYHNGQ39/Zeng et al. - 2020 - A Survey on Machine Reading Comprehension Tasks, .pdf;/Users/ajaynarayanan/Zotero/storage/GTQV4KG4/2006.html}
}

@misc{zhangGettingMoreLess2024,
  title = {Getting {{More}} from {{Less}}: {{Large Language Models}} Are {{Good Spontaneous Multilingual Learners}}},
  shorttitle = {Getting {{More}} from {{Less}}},
  author = {Zhang, Shimao and Gao, Changjiang and Zhu, Wenhao and Chen, Jiajun and Huang, Xin and Han, Xue and Feng, Junlan and Deng, Chao and Huang, Shujian},
  year = {2024},
  month = jun,
  number = {arXiv:2405.13816},
  eprint = {2405.13816},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.13816},
  urldate = {2024-09-02},
  abstract = {Recently, Large Language Models (LLMs) have shown impressive language capabilities. While most of the existing LLMs have very unbalanced performance across different languages, multilingual alignment based on translation parallel data is an effective method to enhance the LLMs' multilingual capabilities. In this work, we discover and comprehensively investigate the spontaneous multilingual alignment improvement of LLMs. We find that LLMs instruction-tuned on the question translation data (i.e. without annotated answers) are able to encourage the alignment between English and a wide range of languages, even including those unseen during instruction-tuning. Additionally, we utilize different settings and mechanistic interpretability methods to analyze the LLM's performance in the multilingual scenario comprehensively. Our work suggests that LLMs have enormous potential for improving multilingual alignment efficiently with great language and task generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/6THFF4AI/Zhang et al. - 2024 - Getting More from Less Large Language Models are .pdf;/Users/ajaynarayanan/Zotero/storage/22T268VK/2405.html}
}

@misc{zhaoExplainabilityLargeLanguage2023,
  title = {Explainability for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Explainability for {{Large Language Models}}},
  author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  year = {2023},
  month = nov,
  number = {arXiv:2309.01029},
  eprint = {2309.01029},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.01029},
  urldate = {2024-08-18},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ajaynarayanan/Zotero/storage/HA843B2Z/Zhao et al. - 2023 - Explainability for Large Language Models A Survey.pdf;/Users/ajaynarayanan/Zotero/storage/DJB2JYVL/2309.html}
}

@misc{zouSurveyDiffusionModels2023,
  title = {A {{Survey}} of {{Diffusion Models}} in {{Natural Language Processing}}},
  author = {Zou, Hao and Kim, Zae Myung and Kang, Dongyeop},
  year = {2023},
  month = jun,
  number = {arXiv:2305.14671},
  eprint = {2305.14671},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14671},
  urldate = {2024-09-02},
  abstract = {This survey paper provides a comprehensive review of the use of diffusion models in natural language processing (NLP). Diffusion models are a class of mathematical models that aim to capture the diffusion of information or signals across a network or manifold. In NLP, diffusion models have been used in a variety of applications, such as natural language generation, sentiment analysis, topic modeling, and machine translation. This paper discusses the different formulations of diffusion models used in NLP, their strengths and limitations, and their applications. We also perform a thorough comparison between diffusion models and alternative generative models, specifically highlighting the autoregressive (AR) models, while also examining how diverse architectures incorporate the Transformer in conjunction with diffusion models. Compared to AR models, diffusion models have significant advantages for parallel generation, text interpolation, token-level controls such as syntactic structures and semantic contents, and robustness. Exploring further permutations of integrating Transformers into diffusion models would be a valuable pursuit. Also, the development of multimodal diffusion models and large-scale diffusion language models with notable capabilities for few-shot learning would be important directions for the future advance of diffusion models in NLP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ajaynarayanan/Zotero/storage/BFDJR2AS/Zou et al. - 2023 - A Survey of Diffusion Models in Natural Language P.pdf;/Users/ajaynarayanan/Zotero/storage/URLJIHDI/2305.html}
}
